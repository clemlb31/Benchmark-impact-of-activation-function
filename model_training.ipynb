{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from brevage_sales import brevage_preprocessing, Brevage_model\n",
    "from animal10 import animals10_preprocessing, Animals10_model\n",
    "from training_functions import train_model, evaluate_model\n",
    "from mnist import mnist_preprocessing, mnist_model\n",
    "from student_performance import student_model, student_preprocessing\n",
    "from food_price import preprocess, lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downloading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_datasets = [\"umitka/food-price-inflation\",\n",
    "            \"minahilfatima12328/performance-trends-in-education\",\n",
    "            \"alessiocorrado99/animals10\",\n",
    "            \"sebastianwillmann/beverage-sales\"]\n",
    "data_dir = \"data/\"\n",
    "\n",
    "download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/umitka/food-price-inflation\n",
      "License(s): world-bank\n",
      "Downloading food-price-inflation.zip to data//umitka/food-price-inflation\n",
      "100%|████████████████████████████████████████| 462k/462k [00:00<00:00, 1.09MB/s]\n",
      "100%|████████████████████████████████████████| 462k/462k [00:00<00:00, 1.09MB/s]\n",
      "Dataset URL: https://www.kaggle.com/datasets/minahilfatima12328/performance-trends-in-education\n",
      "License(s): CC0-1.0\n",
      "Downloading performance-trends-in-education.zip to data//minahilfatima12328/performance-trends-in-education\n",
      "100%|███████████████████████████████████████| 93.9k/93.9k [00:00<00:00, 538kB/s]\n",
      "100%|███████████████████████████████████████| 93.9k/93.9k [00:00<00:00, 537kB/s]\n",
      "Dataset URL: https://www.kaggle.com/datasets/alessiocorrado99/animals10\n",
      "License(s): GPL-2.0\n",
      "Downloading animals10.zip to data//alessiocorrado99/animals10\n",
      "100%|███████████████████████████████████████▊| 583M/586M [00:16<00:00, 36.7MB/s]\n",
      "100%|████████████████████████████████████████| 586M/586M [00:16<00:00, 37.0MB/s]\n",
      "Dataset URL: https://www.kaggle.com/datasets/sebastianwillmann/beverage-sales\n",
      "License(s): MIT\n",
      "Downloading beverage-sales.zip to data//sebastianwillmann/beverage-sales\n",
      "100%|████████████████████████████████████████| 119M/119M [00:03<00:00, 43.6MB/s]\n",
      "100%|████████████████████████████████████████| 119M/119M [00:03<00:00, 38.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "if download:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    for dataset in kaggle_datasets:\n",
    "        if not os.path.exists(os.path.join(data_dir, dataset.split(\"/\")[-1])):    \n",
    "            os.makedirs(os.path.join(data_dir, dataset.split(\"/\")[-1]), exist_ok=True)\n",
    "            !kaggle datasets download -d {dataset} -p {data_dir}/{dataset} --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_search(param_dict):\n",
    "\n",
    "    keys = param_dict.keys()\n",
    "    values = param_dict.values()\n",
    "    \n",
    "    for combination_of_values in itertools.product(*values):\n",
    "        yield dict(zip(keys, combination_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BREVAGE = True\n",
    "TRAIN_ANIMALS = False # training beaucoup trop long, fonctionnel mais on a pas la puissance necessaire pour le trainter, seulement 7 entrainements fait pour la visualisation\n",
    "TRAIN_MNIST = True\n",
    "TRAIN_STUDENTS = True\n",
    "TRAIN_FOOD = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brevage price forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "brevage_df = pd.read_csv('./data/sebastianwillmann/beverage-sales/synthetic_beverage_sales_data.csv')\n",
    "# on ne garde que 1 000 000 lignes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevage_model_training(brevage_df, learning_rate, num_epochs, batch_size, mode, random_state, use_batch_norm):\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    brevage_df = brevage_df.copy()\n",
    "    brevage_df = brevage_df.sample(n=1000000, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = brevage_preprocessing(brevage_df,test_size=0.2,val_size=0.2,random_state=random_state)\n",
    "    brevage_model = Brevage_model(train_dataset.count_features(), mode=mode,use_batch_norm = use_batch_norm)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(brevage_model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(brevage_model.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    history = train_model(brevage_model, criterion, optimizer, num_epochs,train_loader, val_loader)\n",
    "    test_results = evaluate_model(brevage_model, test_loader, device=torch.device(\"cpu\"),loss_type = 'mse')\n",
    "    print(test_results)\n",
    "\n",
    "    history['final_test_loss'] = test_results\n",
    "    \n",
    "    history['dataset'] = 'brevage'\n",
    "    history['random_state'] = random_state\n",
    "    return history, brevage_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "brevage_model_param = {\n",
    "    'mode' : ['relu','gelu'],\n",
    "    'batch_size' : [64,2048],\n",
    "    'random_state' : [1,2,3],\n",
    "    'use_batch_norm' : [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BREVAGE:\n",
    "    histories = []       \n",
    "    for param in grid_search(brevage_model_param):\n",
    "        hist, brevage_model_ = brevage_model_training(brevage_df, learning_rate=0.001, num_epochs=100, batch_size=param['batch_size'], mode=param['mode'], random_state=param['random_state'], use_batch_norm=param['use_batch_norm'])\n",
    "        histories.append(hist)\n",
    "    json.dump(histories, open('results/brevage_histories.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animals prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals_path = \"data/alessiocorrado99/animals10/raw-img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animals_model_training(animals_path, learning_rate, num_epochs, batch_size, mode, random_state, use_batch_norm):\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset, class_to_idx, idx_to_class = animals10_preprocessing(animals_path, test_size=0.2, val_size=0.2, image_size=128, random_state=random_state, subset=0.5)\n",
    "    animal_model = Animals10_model(num_classes=len(class_to_idx), mode=mode,use_batch_norm=use_batch_norm).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(animal_model.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    history = train_model(animal_model, criterion, optimizer, num_epochs,train_loader, val_loader, device)\n",
    "    test_results = evaluate_model(animal_model, test_loader, device, loss_type = 'cross_entropy')\n",
    "    history['final_test_loss'] = test_results\n",
    "    history['random_state'] = random_state\n",
    "\n",
    "    history['dataset'] = 'animals'\n",
    "    return history, animal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals_model_param = {\n",
    "    'mode' : ['relu','gelu'],\n",
    "    'batch_size' : [128,1024],\n",
    "    'random_state' : [1,2,3],\n",
    "    'use_batch_norm' : [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_ANIMALS:\n",
    "    histories = []       \n",
    "    for param in grid_search(animals_model_param):\n",
    "        hist, animal_model_ = animals_model_training(animals_path, learning_rate=0.001, num_epochs=30, batch_size=param['batch_size'], mode=param['mode'], random_state=param['random_state'], use_batch_norm=param['use_batch_norm'])\n",
    "        histories.append(hist)\n",
    "    json.dump(histories, open('results/animals_histories.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist download\n",
    "(mnist_X_train_full, mnist_y_train_full), (mnist_X_test, mnist_y_test) = (keras.datasets.mnist.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_model_training(mnist_X_train_full, mnist_y_train_full, mnist_X_test, mnist_y_test, learning_rate, num_epochs, batch_size, mode, random_state, use_batch_norm):\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset, scaler = mnist_preprocessing(mnist_X_train_full, mnist_y_train_full, mnist_X_test, mnist_y_test, val_size=0.2, random_state=random_state)\n",
    "    mnist_model_ = mnist_model(mode=mode, use_batch_norm=use_batch_norm).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(mnist_model_.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    history = train_model(mnist_model_, criterion, optimizer, num_epochs,train_loader, val_loader, device)\n",
    "    test_results = evaluate_model(mnist_model_, test_loader, device, loss_type = 'cross_entropy')\n",
    "    history['final_test_loss'] = test_results\n",
    "    history['random_state'] = random_state\n",
    "\n",
    "    history['dataset'] = 'mnist'\n",
    "    return history, mnist_model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model_param = {\n",
    "    'mode' : ['relu','gelu'],\n",
    "    'batch_size' : [64,1024],\n",
    "    'random_state' : [1,2,3],\n",
    "    'use_batch_norm' : [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MNIST:\n",
    "    histories = []       \n",
    "    for param in grid_search(mnist_model_param):\n",
    "        hist, mnist_model_ = mnist_model_training(mnist_X_train_full, mnist_y_train_full, mnist_X_test, mnist_y_test, learning_rate=0.001, num_epochs=40, batch_size=param['batch_size'], mode=param['mode'], random_state=param['random_state'], use_batch_norm=param['use_batch_norm'])\n",
    "        histories.append(hist)\n",
    "    json.dump(histories, open('results/mnist_histories.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Grade Forcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = pd.read_csv('./data/minahilfatima12328/performance-trends-in-education/StudentPerformanceFactors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_model_training(student_df, learning_rate, num_epochs, batch_size, mode, random_state, use_batch_norm):\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    " \n",
    "    train_dataset, val_dataset, test_dataset, scaler_X, scaler_y = student_preprocessing(student_df, val_size=0.2, test_size=0.2, random_state=random_state)\n",
    "    input_size = train_dataset.tensors[0].shape[1]\n",
    "    student_model_ = student_model(input_dim=input_size, mode=mode, use_batch_norm=use_batch_norm).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(student_model_.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    history = train_model(student_model_, criterion, optimizer, num_epochs,train_loader, val_loader, device)\n",
    "    test_results = evaluate_model(student_model_, test_loader, device,loss_type='mse')\n",
    "    history['final_test_loss'] = test_results\n",
    "    history['dataset'] = 'student'\n",
    "    history['random_state'] = random_state\n",
    "\n",
    "    return history, student_model_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_param = {\n",
    "    'mode' : ['relu','gelu'],\n",
    "    'batch_size' : [32,1024],\n",
    "    'random_state' : [1,2,3],\n",
    "    'use_batch_norm' : [True, False]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.7774, Validation Loss: 0.5014\n",
      "Epoch 2/100, Training Loss: 0.4942, Validation Loss: 0.4585\n",
      "Epoch 3/100, Training Loss: 0.4151, Validation Loss: 0.4426\n",
      "Epoch 4/100, Training Loss: 0.3846, Validation Loss: 0.4459\n",
      "Epoch 5/100, Training Loss: 0.3687, Validation Loss: 0.4328\n",
      "Epoch 6/100, Training Loss: 0.3616, Validation Loss: 0.4369\n",
      "Epoch 7/100, Training Loss: 0.3593, Validation Loss: 0.4346\n",
      "Epoch 8/100, Training Loss: 0.3567, Validation Loss: 0.4424\n",
      "Epoch 9/100, Training Loss: 0.3408, Validation Loss: 0.4358\n",
      "Epoch 10/100, Training Loss: 0.3344, Validation Loss: 0.4223\n",
      "Epoch 11/100, Training Loss: 0.3409, Validation Loss: 0.4227\n",
      "Epoch 12/100, Training Loss: 0.3449, Validation Loss: 0.4199\n",
      "Epoch 13/100, Training Loss: 0.3401, Validation Loss: 0.4388\n",
      "Epoch 14/100, Training Loss: 0.3383, Validation Loss: 0.4299\n",
      "Epoch 15/100, Training Loss: 0.3307, Validation Loss: 0.4305\n",
      "Epoch 16/100, Training Loss: 0.3241, Validation Loss: 0.4233\n",
      "Epoch 17/100, Training Loss: 0.3314, Validation Loss: 0.4241\n",
      "Epoch 18/100, Training Loss: 0.3328, Validation Loss: 0.4269\n",
      "Epoch 19/100, Training Loss: 0.3295, Validation Loss: 0.4238\n",
      "Epoch 20/100, Training Loss: 0.3298, Validation Loss: 0.4247\n",
      "Epoch 21/100, Training Loss: 0.3295, Validation Loss: 0.4270\n",
      "Epoch 22/100, Training Loss: 0.3270, Validation Loss: 0.4275\n",
      "Epoch 23/100, Training Loss: 0.3182, Validation Loss: 0.4206\n",
      "Epoch 24/100, Training Loss: 0.3212, Validation Loss: 0.4208\n",
      "Epoch 25/100, Training Loss: 0.3233, Validation Loss: 0.4304\n",
      "Epoch 26/100, Training Loss: 0.3111, Validation Loss: 0.4262\n",
      "Epoch 27/100, Training Loss: 0.3167, Validation Loss: 0.4293\n",
      "Epoch 28/100, Training Loss: 0.3156, Validation Loss: 0.4246\n",
      "Epoch 29/100, Training Loss: 0.3307, Validation Loss: 0.4223\n",
      "Epoch 30/100, Training Loss: 0.3233, Validation Loss: 0.4229\n",
      "Epoch 31/100, Training Loss: 0.3289, Validation Loss: 0.4316\n",
      "Epoch 32/100, Training Loss: 0.3185, Validation Loss: 0.4254\n",
      "Epoch 33/100, Training Loss: 0.3102, Validation Loss: 0.4225\n",
      "Epoch 34/100, Training Loss: 0.3224, Validation Loss: 0.4288\n",
      "Epoch 35/100, Training Loss: 0.3200, Validation Loss: 0.4226\n",
      "Epoch 36/100, Training Loss: 0.3126, Validation Loss: 0.4251\n",
      "Epoch 37/100, Training Loss: 0.3258, Validation Loss: 0.4279\n",
      "Epoch 38/100, Training Loss: 0.3242, Validation Loss: 0.4203\n",
      "Epoch 39/100, Training Loss: 0.3299, Validation Loss: 0.4245\n",
      "Epoch 40/100, Training Loss: 0.3194, Validation Loss: 0.4355\n",
      "Epoch 41/100, Training Loss: 0.3173, Validation Loss: 0.4237\n",
      "Epoch 42/100, Training Loss: 0.3226, Validation Loss: 0.4193\n",
      "Epoch 43/100, Training Loss: 0.3139, Validation Loss: 0.4216\n",
      "Epoch 44/100, Training Loss: 0.3186, Validation Loss: 0.4252\n",
      "Epoch 45/100, Training Loss: 0.3201, Validation Loss: 0.4253\n",
      "Epoch 46/100, Training Loss: 0.3142, Validation Loss: 0.4212\n",
      "Epoch 47/100, Training Loss: 0.3116, Validation Loss: 0.4185\n",
      "Epoch 48/100, Training Loss: 0.3073, Validation Loss: 0.4246\n",
      "Epoch 49/100, Training Loss: 0.3158, Validation Loss: 0.4229\n",
      "Epoch 50/100, Training Loss: 0.3176, Validation Loss: 0.4301\n",
      "Epoch 51/100, Training Loss: 0.3235, Validation Loss: 0.4249\n",
      "Epoch 52/100, Training Loss: 0.3146, Validation Loss: 0.4217\n",
      "Epoch 53/100, Training Loss: 0.3132, Validation Loss: 0.4268\n",
      "Epoch 54/100, Training Loss: 0.3051, Validation Loss: 0.4202\n",
      "Epoch 55/100, Training Loss: 0.3210, Validation Loss: 0.4238\n",
      "Epoch 56/100, Training Loss: 0.3138, Validation Loss: 0.4256\n",
      "Epoch 57/100, Training Loss: 0.3171, Validation Loss: 0.4205\n",
      "Epoch 58/100, Training Loss: 0.3166, Validation Loss: 0.4269\n",
      "Epoch 59/100, Training Loss: 0.3179, Validation Loss: 0.4243\n",
      "Epoch 60/100, Training Loss: 0.3097, Validation Loss: 0.4276\n",
      "Epoch 61/100, Training Loss: 0.3108, Validation Loss: 0.4348\n",
      "Epoch 62/100, Training Loss: 0.3194, Validation Loss: 0.4258\n",
      "Epoch 63/100, Training Loss: 0.3199, Validation Loss: 0.4251\n",
      "Epoch 64/100, Training Loss: 0.3109, Validation Loss: 0.4319\n",
      "Epoch 65/100, Training Loss: 0.2998, Validation Loss: 0.4266\n",
      "Epoch 66/100, Training Loss: 0.3189, Validation Loss: 0.4236\n",
      "Epoch 67/100, Training Loss: 0.3125, Validation Loss: 0.4308\n",
      "Epoch 68/100, Training Loss: 0.3247, Validation Loss: 0.4377\n",
      "Epoch 69/100, Training Loss: 0.3090, Validation Loss: 0.4325\n",
      "Epoch 70/100, Training Loss: 0.3062, Validation Loss: 0.4269\n",
      "Epoch 71/100, Training Loss: 0.3158, Validation Loss: 0.4250\n",
      "Epoch 72/100, Training Loss: 0.3105, Validation Loss: 0.4262\n",
      "Epoch 73/100, Training Loss: 0.3083, Validation Loss: 0.4295\n",
      "Epoch 74/100, Training Loss: 0.3200, Validation Loss: 0.4249\n",
      "Epoch 75/100, Training Loss: 0.3201, Validation Loss: 0.4275\n",
      "Epoch 76/100, Training Loss: 0.3120, Validation Loss: 0.4232\n",
      "Epoch 77/100, Training Loss: 0.3032, Validation Loss: 0.4239\n",
      "Epoch 78/100, Training Loss: 0.3087, Validation Loss: 0.4372\n",
      "Epoch 79/100, Training Loss: 0.3112, Validation Loss: 0.4257\n",
      "Epoch 80/100, Training Loss: 0.3027, Validation Loss: 0.4391\n",
      "Epoch 81/100, Training Loss: 0.3032, Validation Loss: 0.4346\n",
      "Epoch 82/100, Training Loss: 0.3085, Validation Loss: 0.4263\n",
      "Epoch 83/100, Training Loss: 0.3045, Validation Loss: 0.4277\n",
      "Epoch 84/100, Training Loss: 0.3188, Validation Loss: 0.4281\n",
      "Epoch 85/100, Training Loss: 0.3037, Validation Loss: 0.4231\n",
      "Epoch 86/100, Training Loss: 0.2990, Validation Loss: 0.4292\n",
      "Epoch 87/100, Training Loss: 0.3163, Validation Loss: 0.4238\n",
      "Epoch 88/100, Training Loss: 0.3100, Validation Loss: 0.4333\n",
      "Epoch 89/100, Training Loss: 0.3145, Validation Loss: 0.4278\n",
      "Epoch 90/100, Training Loss: 0.3129, Validation Loss: 0.4320\n",
      "Epoch 91/100, Training Loss: 0.3218, Validation Loss: 0.4224\n",
      "Epoch 92/100, Training Loss: 0.3018, Validation Loss: 0.4226\n",
      "Epoch 93/100, Training Loss: 0.3044, Validation Loss: 0.4285\n",
      "Epoch 94/100, Training Loss: 0.3117, Validation Loss: 0.4237\n",
      "Epoch 95/100, Training Loss: 0.3060, Validation Loss: 0.4261\n",
      "Epoch 96/100, Training Loss: 0.2998, Validation Loss: 0.4264\n",
      "Epoch 97/100, Training Loss: 0.3092, Validation Loss: 0.4353\n",
      "Epoch 98/100, Training Loss: 0.3040, Validation Loss: 0.4217\n",
      "Epoch 99/100, Training Loss: 0.3130, Validation Loss: 0.4290\n",
      "Epoch 100/100, Training Loss: 0.3045, Validation Loss: 0.4256\n",
      "Epoch 1/100, Training Loss: 0.6754, Validation Loss: 0.4661\n",
      "Epoch 2/100, Training Loss: 0.4022, Validation Loss: 0.4336\n",
      "Epoch 3/100, Training Loss: 0.3492, Validation Loss: 0.4263\n",
      "Epoch 4/100, Training Loss: 0.3337, Validation Loss: 0.4348\n",
      "Epoch 5/100, Training Loss: 0.3225, Validation Loss: 0.4233\n",
      "Epoch 6/100, Training Loss: 0.3191, Validation Loss: 0.4225\n",
      "Epoch 7/100, Training Loss: 0.3182, Validation Loss: 0.4280\n",
      "Epoch 8/100, Training Loss: 0.3134, Validation Loss: 0.4274\n",
      "Epoch 9/100, Training Loss: 0.3042, Validation Loss: 0.4239\n",
      "Epoch 10/100, Training Loss: 0.3037, Validation Loss: 0.4225\n",
      "Epoch 11/100, Training Loss: 0.3041, Validation Loss: 0.4244\n",
      "Epoch 12/100, Training Loss: 0.3023, Validation Loss: 0.4209\n",
      "Epoch 13/100, Training Loss: 0.2955, Validation Loss: 0.4372\n",
      "Epoch 14/100, Training Loss: 0.3009, Validation Loss: 0.4225\n",
      "Epoch 15/100, Training Loss: 0.2970, Validation Loss: 0.4298\n",
      "Epoch 16/100, Training Loss: 0.2924, Validation Loss: 0.4212\n",
      "Epoch 17/100, Training Loss: 0.2925, Validation Loss: 0.4206\n",
      "Epoch 18/100, Training Loss: 0.2905, Validation Loss: 0.4244\n",
      "Epoch 19/100, Training Loss: 0.2961, Validation Loss: 0.4208\n",
      "Epoch 20/100, Training Loss: 0.2910, Validation Loss: 0.4194\n",
      "Epoch 21/100, Training Loss: 0.2867, Validation Loss: 0.4207\n",
      "Epoch 22/100, Training Loss: 0.2883, Validation Loss: 0.4199\n",
      "Epoch 23/100, Training Loss: 0.2894, Validation Loss: 0.4188\n",
      "Epoch 24/100, Training Loss: 0.2871, Validation Loss: 0.4171\n",
      "Epoch 25/100, Training Loss: 0.2880, Validation Loss: 0.4228\n",
      "Epoch 26/100, Training Loss: 0.2796, Validation Loss: 0.4246\n",
      "Epoch 27/100, Training Loss: 0.2801, Validation Loss: 0.4202\n",
      "Epoch 28/100, Training Loss: 0.2792, Validation Loss: 0.4198\n",
      "Epoch 29/100, Training Loss: 0.2802, Validation Loss: 0.4176\n",
      "Epoch 30/100, Training Loss: 0.2864, Validation Loss: 0.4173\n",
      "Epoch 31/100, Training Loss: 0.2845, Validation Loss: 0.4283\n",
      "Epoch 32/100, Training Loss: 0.2905, Validation Loss: 0.4228\n",
      "Epoch 33/100, Training Loss: 0.2725, Validation Loss: 0.4182\n",
      "Epoch 34/100, Training Loss: 0.2860, Validation Loss: 0.4200\n",
      "Epoch 35/100, Training Loss: 0.2782, Validation Loss: 0.4277\n",
      "Epoch 36/100, Training Loss: 0.2796, Validation Loss: 0.4250\n",
      "Epoch 37/100, Training Loss: 0.2890, Validation Loss: 0.4234\n",
      "Epoch 38/100, Training Loss: 0.2878, Validation Loss: 0.4209\n",
      "Epoch 39/100, Training Loss: 0.2758, Validation Loss: 0.4205\n",
      "Epoch 40/100, Training Loss: 0.2795, Validation Loss: 0.4220\n",
      "Epoch 41/100, Training Loss: 0.2756, Validation Loss: 0.4252\n",
      "Epoch 42/100, Training Loss: 0.2769, Validation Loss: 0.4197\n",
      "Epoch 43/100, Training Loss: 0.2755, Validation Loss: 0.4174\n",
      "Epoch 44/100, Training Loss: 0.2820, Validation Loss: 0.4187\n",
      "Epoch 45/100, Training Loss: 0.2763, Validation Loss: 0.4218\n",
      "Epoch 46/100, Training Loss: 0.2807, Validation Loss: 0.4225\n",
      "Epoch 47/100, Training Loss: 0.2781, Validation Loss: 0.4209\n",
      "Epoch 48/100, Training Loss: 0.2707, Validation Loss: 0.4237\n",
      "Epoch 49/100, Training Loss: 0.2713, Validation Loss: 0.4220\n",
      "Epoch 50/100, Training Loss: 0.2774, Validation Loss: 0.4283\n",
      "Epoch 51/100, Training Loss: 0.2783, Validation Loss: 0.4221\n",
      "Epoch 52/100, Training Loss: 0.2747, Validation Loss: 0.4198\n",
      "Epoch 53/100, Training Loss: 0.2708, Validation Loss: 0.4216\n",
      "Epoch 54/100, Training Loss: 0.2658, Validation Loss: 0.4220\n",
      "Epoch 55/100, Training Loss: 0.2818, Validation Loss: 0.4233\n",
      "Epoch 56/100, Training Loss: 0.2752, Validation Loss: 0.4217\n",
      "Epoch 57/100, Training Loss: 0.2674, Validation Loss: 0.4247\n",
      "Epoch 58/100, Training Loss: 0.2758, Validation Loss: 0.4255\n",
      "Epoch 59/100, Training Loss: 0.2686, Validation Loss: 0.4237\n",
      "Epoch 60/100, Training Loss: 0.2723, Validation Loss: 0.4220\n",
      "Epoch 61/100, Training Loss: 0.2673, Validation Loss: 0.4290\n",
      "Epoch 62/100, Training Loss: 0.2677, Validation Loss: 0.4234\n",
      "Epoch 63/100, Training Loss: 0.2710, Validation Loss: 0.4205\n",
      "Epoch 64/100, Training Loss: 0.2715, Validation Loss: 0.4197\n",
      "Epoch 65/100, Training Loss: 0.2592, Validation Loss: 0.4260\n",
      "Epoch 66/100, Training Loss: 0.2637, Validation Loss: 0.4227\n",
      "Epoch 67/100, Training Loss: 0.2625, Validation Loss: 0.4264\n",
      "Epoch 68/100, Training Loss: 0.2753, Validation Loss: 0.4315\n",
      "Epoch 69/100, Training Loss: 0.2642, Validation Loss: 0.4291\n",
      "Epoch 70/100, Training Loss: 0.2644, Validation Loss: 0.4218\n",
      "Epoch 71/100, Training Loss: 0.2727, Validation Loss: 0.4277\n",
      "Epoch 72/100, Training Loss: 0.2649, Validation Loss: 0.4252\n",
      "Epoch 73/100, Training Loss: 0.2636, Validation Loss: 0.4303\n",
      "Epoch 74/100, Training Loss: 0.2644, Validation Loss: 0.4216\n",
      "Epoch 75/100, Training Loss: 0.2681, Validation Loss: 0.4220\n",
      "Epoch 76/100, Training Loss: 0.2591, Validation Loss: 0.4235\n",
      "Epoch 77/100, Training Loss: 0.2548, Validation Loss: 0.4260\n",
      "Epoch 78/100, Training Loss: 0.2693, Validation Loss: 0.4297\n",
      "Epoch 79/100, Training Loss: 0.2643, Validation Loss: 0.4270\n",
      "Epoch 80/100, Training Loss: 0.2492, Validation Loss: 0.4250\n",
      "Epoch 81/100, Training Loss: 0.2643, Validation Loss: 0.4240\n",
      "Epoch 82/100, Training Loss: 0.2516, Validation Loss: 0.4262\n",
      "Epoch 83/100, Training Loss: 0.2565, Validation Loss: 0.4280\n",
      "Epoch 84/100, Training Loss: 0.2607, Validation Loss: 0.4318\n",
      "Epoch 85/100, Training Loss: 0.2552, Validation Loss: 0.4234\n",
      "Epoch 86/100, Training Loss: 0.2513, Validation Loss: 0.4312\n",
      "Epoch 87/100, Training Loss: 0.2639, Validation Loss: 0.4204\n",
      "Epoch 88/100, Training Loss: 0.2547, Validation Loss: 0.4257\n",
      "Epoch 89/100, Training Loss: 0.2579, Validation Loss: 0.4285\n",
      "Epoch 90/100, Training Loss: 0.2471, Validation Loss: 0.4301\n",
      "Epoch 91/100, Training Loss: 0.2572, Validation Loss: 0.4313\n",
      "Epoch 92/100, Training Loss: 0.2446, Validation Loss: 0.4251\n",
      "Epoch 93/100, Training Loss: 0.2688, Validation Loss: 0.4257\n",
      "Epoch 94/100, Training Loss: 0.2463, Validation Loss: 0.4240\n",
      "Epoch 95/100, Training Loss: 0.2492, Validation Loss: 0.4266\n",
      "Epoch 96/100, Training Loss: 0.2464, Validation Loss: 0.4276\n",
      "Epoch 97/100, Training Loss: 0.2459, Validation Loss: 0.4309\n",
      "Epoch 98/100, Training Loss: 0.2517, Validation Loss: 0.4270\n",
      "Epoch 99/100, Training Loss: 0.2533, Validation Loss: 0.4309\n",
      "Epoch 100/100, Training Loss: 0.2453, Validation Loss: 0.4295\n",
      "Epoch 1/100, Training Loss: 0.7637, Validation Loss: 0.3760\n",
      "Epoch 2/100, Training Loss: 0.5018, Validation Loss: 0.2888\n",
      "Epoch 3/100, Training Loss: 0.4382, Validation Loss: 0.2701\n",
      "Epoch 4/100, Training Loss: 0.4060, Validation Loss: 0.2683\n",
      "Epoch 5/100, Training Loss: 0.3824, Validation Loss: 0.2496\n",
      "Epoch 6/100, Training Loss: 0.3841, Validation Loss: 0.2483\n",
      "Epoch 7/100, Training Loss: 0.3715, Validation Loss: 0.2549\n",
      "Epoch 8/100, Training Loss: 0.3791, Validation Loss: 0.2527\n",
      "Epoch 9/100, Training Loss: 0.3700, Validation Loss: 0.2563\n",
      "Epoch 10/100, Training Loss: 0.3580, Validation Loss: 0.2530\n",
      "Epoch 11/100, Training Loss: 0.3495, Validation Loss: 0.2497\n",
      "Epoch 12/100, Training Loss: 0.3498, Validation Loss: 0.2404\n",
      "Epoch 13/100, Training Loss: 0.3382, Validation Loss: 0.2413\n",
      "Epoch 14/100, Training Loss: 0.3578, Validation Loss: 0.2364\n",
      "Epoch 15/100, Training Loss: 0.3387, Validation Loss: 0.2373\n",
      "Epoch 16/100, Training Loss: 0.3497, Validation Loss: 0.2513\n",
      "Epoch 17/100, Training Loss: 0.3414, Validation Loss: 0.2354\n",
      "Epoch 18/100, Training Loss: 0.3370, Validation Loss: 0.2398\n",
      "Epoch 19/100, Training Loss: 0.3557, Validation Loss: 0.2443\n",
      "Epoch 20/100, Training Loss: 0.3372, Validation Loss: 0.2373\n",
      "Epoch 21/100, Training Loss: 0.3506, Validation Loss: 0.2355\n",
      "Epoch 22/100, Training Loss: 0.3433, Validation Loss: 0.2450\n",
      "Epoch 23/100, Training Loss: 0.3367, Validation Loss: 0.2360\n",
      "Epoch 24/100, Training Loss: 0.3373, Validation Loss: 0.2416\n",
      "Epoch 25/100, Training Loss: 0.3527, Validation Loss: 0.2374\n",
      "Epoch 26/100, Training Loss: 0.3420, Validation Loss: 0.2369\n",
      "Epoch 27/100, Training Loss: 0.3388, Validation Loss: 0.2344\n",
      "Epoch 28/100, Training Loss: 0.3343, Validation Loss: 0.2379\n",
      "Epoch 29/100, Training Loss: 0.3312, Validation Loss: 0.2426\n",
      "Epoch 30/100, Training Loss: 0.3392, Validation Loss: 0.2525\n",
      "Epoch 31/100, Training Loss: 0.3351, Validation Loss: 0.2418\n",
      "Epoch 32/100, Training Loss: 0.3386, Validation Loss: 0.2608\n",
      "Epoch 33/100, Training Loss: 0.3337, Validation Loss: 0.2426\n",
      "Epoch 34/100, Training Loss: 0.3364, Validation Loss: 0.2413\n",
      "Epoch 35/100, Training Loss: 0.3456, Validation Loss: 0.2349\n",
      "Epoch 36/100, Training Loss: 0.3407, Validation Loss: 0.2405\n",
      "Epoch 37/100, Training Loss: 0.3290, Validation Loss: 0.2457\n",
      "Epoch 38/100, Training Loss: 0.3336, Validation Loss: 0.2362\n",
      "Epoch 39/100, Training Loss: 0.3330, Validation Loss: 0.2453\n",
      "Epoch 40/100, Training Loss: 0.3298, Validation Loss: 0.2358\n",
      "Epoch 41/100, Training Loss: 0.3466, Validation Loss: 0.2421\n",
      "Epoch 42/100, Training Loss: 0.3316, Validation Loss: 0.2399\n",
      "Epoch 43/100, Training Loss: 0.3317, Validation Loss: 0.2441\n",
      "Epoch 44/100, Training Loss: 0.3349, Validation Loss: 0.2378\n",
      "Epoch 45/100, Training Loss: 0.3459, Validation Loss: 0.2486\n",
      "Epoch 46/100, Training Loss: 0.3390, Validation Loss: 0.2509\n",
      "Epoch 47/100, Training Loss: 0.3363, Validation Loss: 0.2405\n",
      "Epoch 48/100, Training Loss: 0.3282, Validation Loss: 0.2376\n",
      "Epoch 49/100, Training Loss: 0.3356, Validation Loss: 0.2401\n",
      "Epoch 50/100, Training Loss: 0.3341, Validation Loss: 0.2326\n",
      "Epoch 51/100, Training Loss: 0.3284, Validation Loss: 0.2363\n",
      "Epoch 52/100, Training Loss: 0.3336, Validation Loss: 0.2389\n",
      "Epoch 53/100, Training Loss: 0.3378, Validation Loss: 0.2497\n",
      "Epoch 54/100, Training Loss: 0.3351, Validation Loss: 0.2382\n",
      "Epoch 55/100, Training Loss: 0.3345, Validation Loss: 0.2499\n",
      "Epoch 56/100, Training Loss: 0.3276, Validation Loss: 0.2369\n",
      "Epoch 57/100, Training Loss: 0.3281, Validation Loss: 0.2387\n",
      "Epoch 58/100, Training Loss: 0.3454, Validation Loss: 0.2370\n",
      "Epoch 59/100, Training Loss: 0.3378, Validation Loss: 0.2425\n",
      "Epoch 60/100, Training Loss: 0.3372, Validation Loss: 0.2329\n",
      "Epoch 61/100, Training Loss: 0.3245, Validation Loss: 0.2370\n",
      "Epoch 62/100, Training Loss: 0.3327, Validation Loss: 0.2330\n",
      "Epoch 63/100, Training Loss: 0.3350, Validation Loss: 0.2532\n",
      "Epoch 64/100, Training Loss: 0.3317, Validation Loss: 0.2476\n",
      "Epoch 65/100, Training Loss: 0.3430, Validation Loss: 0.2353\n",
      "Epoch 66/100, Training Loss: 0.3281, Validation Loss: 0.2373\n",
      "Epoch 67/100, Training Loss: 0.3324, Validation Loss: 0.2416\n",
      "Epoch 68/100, Training Loss: 0.3313, Validation Loss: 0.2361\n",
      "Epoch 69/100, Training Loss: 0.3374, Validation Loss: 0.2507\n",
      "Epoch 70/100, Training Loss: 0.3374, Validation Loss: 0.2434\n",
      "Epoch 71/100, Training Loss: 0.3307, Validation Loss: 0.2402\n",
      "Epoch 72/100, Training Loss: 0.3247, Validation Loss: 0.2422\n",
      "Epoch 73/100, Training Loss: 0.3369, Validation Loss: 0.2383\n",
      "Epoch 74/100, Training Loss: 0.3356, Validation Loss: 0.2465\n",
      "Epoch 75/100, Training Loss: 0.3333, Validation Loss: 0.2395\n",
      "Epoch 76/100, Training Loss: 0.3308, Validation Loss: 0.2357\n",
      "Epoch 77/100, Training Loss: 0.3281, Validation Loss: 0.2388\n",
      "Epoch 78/100, Training Loss: 0.3358, Validation Loss: 0.2318\n",
      "Epoch 79/100, Training Loss: 0.3240, Validation Loss: 0.2408\n",
      "Epoch 80/100, Training Loss: 0.3259, Validation Loss: 0.2419\n",
      "Epoch 81/100, Training Loss: 0.3362, Validation Loss: 0.2433\n",
      "Epoch 82/100, Training Loss: 0.3295, Validation Loss: 0.2388\n",
      "Epoch 83/100, Training Loss: 0.3347, Validation Loss: 0.2464\n",
      "Epoch 84/100, Training Loss: 0.3247, Validation Loss: 0.2411\n",
      "Epoch 85/100, Training Loss: 0.3272, Validation Loss: 0.2414\n",
      "Epoch 86/100, Training Loss: 0.3395, Validation Loss: 0.2393\n",
      "Epoch 87/100, Training Loss: 0.3322, Validation Loss: 0.2456\n",
      "Epoch 88/100, Training Loss: 0.3242, Validation Loss: 0.2417\n",
      "Epoch 89/100, Training Loss: 0.3320, Validation Loss: 0.2471\n",
      "Epoch 90/100, Training Loss: 0.3356, Validation Loss: 0.2333\n",
      "Epoch 91/100, Training Loss: 0.3319, Validation Loss: 0.2373\n",
      "Epoch 92/100, Training Loss: 0.3258, Validation Loss: 0.2449\n",
      "Epoch 93/100, Training Loss: 0.3310, Validation Loss: 0.2399\n",
      "Epoch 94/100, Training Loss: 0.3319, Validation Loss: 0.2383\n",
      "Epoch 95/100, Training Loss: 0.3294, Validation Loss: 0.2320\n",
      "Epoch 96/100, Training Loss: 0.3280, Validation Loss: 0.2348\n",
      "Epoch 97/100, Training Loss: 0.3339, Validation Loss: 0.2345\n",
      "Epoch 98/100, Training Loss: 0.3363, Validation Loss: 0.2420\n",
      "Epoch 99/100, Training Loss: 0.3343, Validation Loss: 0.2370\n",
      "Epoch 100/100, Training Loss: 0.3303, Validation Loss: 0.2478\n",
      "Epoch 1/100, Training Loss: 0.7068, Validation Loss: 0.3176\n",
      "Epoch 2/100, Training Loss: 0.4186, Validation Loss: 0.2699\n",
      "Epoch 3/100, Training Loss: 0.3706, Validation Loss: 0.2465\n",
      "Epoch 4/100, Training Loss: 0.3590, Validation Loss: 0.2478\n",
      "Epoch 5/100, Training Loss: 0.3446, Validation Loss: 0.2444\n",
      "Epoch 6/100, Training Loss: 0.3463, Validation Loss: 0.2442\n",
      "Epoch 7/100, Training Loss: 0.3318, Validation Loss: 0.2339\n",
      "Epoch 8/100, Training Loss: 0.3359, Validation Loss: 0.2466\n",
      "Epoch 9/100, Training Loss: 0.3296, Validation Loss: 0.2424\n",
      "Epoch 10/100, Training Loss: 0.3200, Validation Loss: 0.2407\n",
      "Epoch 11/100, Training Loss: 0.3184, Validation Loss: 0.2356\n",
      "Epoch 12/100, Training Loss: 0.3176, Validation Loss: 0.2358\n",
      "Epoch 13/100, Training Loss: 0.3111, Validation Loss: 0.2361\n",
      "Epoch 14/100, Training Loss: 0.3216, Validation Loss: 0.2402\n",
      "Epoch 15/100, Training Loss: 0.3087, Validation Loss: 0.2458\n",
      "Epoch 16/100, Training Loss: 0.3139, Validation Loss: 0.2379\n",
      "Epoch 17/100, Training Loss: 0.3080, Validation Loss: 0.2329\n",
      "Epoch 18/100, Training Loss: 0.3072, Validation Loss: 0.2328\n",
      "Epoch 19/100, Training Loss: 0.3234, Validation Loss: 0.2318\n",
      "Epoch 20/100, Training Loss: 0.3094, Validation Loss: 0.2301\n",
      "Epoch 21/100, Training Loss: 0.3133, Validation Loss: 0.2303\n",
      "Epoch 22/100, Training Loss: 0.3138, Validation Loss: 0.2309\n",
      "Epoch 23/100, Training Loss: 0.3067, Validation Loss: 0.2360\n",
      "Epoch 24/100, Training Loss: 0.3088, Validation Loss: 0.2399\n",
      "Epoch 25/100, Training Loss: 0.3096, Validation Loss: 0.2320\n",
      "Epoch 26/100, Training Loss: 0.3053, Validation Loss: 0.2299\n",
      "Epoch 27/100, Training Loss: 0.3038, Validation Loss: 0.2346\n",
      "Epoch 28/100, Training Loss: 0.3077, Validation Loss: 0.2342\n",
      "Epoch 29/100, Training Loss: 0.3051, Validation Loss: 0.2325\n",
      "Epoch 30/100, Training Loss: 0.3031, Validation Loss: 0.2312\n",
      "Epoch 31/100, Training Loss: 0.3039, Validation Loss: 0.2353\n",
      "Epoch 32/100, Training Loss: 0.3060, Validation Loss: 0.2420\n",
      "Epoch 33/100, Training Loss: 0.3028, Validation Loss: 0.2339\n",
      "Epoch 34/100, Training Loss: 0.3032, Validation Loss: 0.2325\n",
      "Epoch 35/100, Training Loss: 0.3054, Validation Loss: 0.2311\n",
      "Epoch 36/100, Training Loss: 0.2968, Validation Loss: 0.2375\n",
      "Epoch 37/100, Training Loss: 0.3019, Validation Loss: 0.2354\n",
      "Epoch 38/100, Training Loss: 0.2985, Validation Loss: 0.2330\n",
      "Epoch 39/100, Training Loss: 0.2942, Validation Loss: 0.2369\n",
      "Epoch 40/100, Training Loss: 0.3004, Validation Loss: 0.2309\n",
      "Epoch 41/100, Training Loss: 0.3061, Validation Loss: 0.2415\n",
      "Epoch 42/100, Training Loss: 0.2963, Validation Loss: 0.2387\n",
      "Epoch 43/100, Training Loss: 0.2917, Validation Loss: 0.2375\n",
      "Epoch 44/100, Training Loss: 0.3037, Validation Loss: 0.2300\n",
      "Epoch 45/100, Training Loss: 0.2986, Validation Loss: 0.2349\n",
      "Epoch 46/100, Training Loss: 0.3031, Validation Loss: 0.2347\n",
      "Epoch 47/100, Training Loss: 0.3040, Validation Loss: 0.2325\n",
      "Epoch 48/100, Training Loss: 0.2927, Validation Loss: 0.2305\n",
      "Epoch 49/100, Training Loss: 0.2961, Validation Loss: 0.2309\n",
      "Epoch 50/100, Training Loss: 0.2977, Validation Loss: 0.2321\n",
      "Epoch 51/100, Training Loss: 0.2917, Validation Loss: 0.2406\n",
      "Epoch 52/100, Training Loss: 0.2993, Validation Loss: 0.2339\n",
      "Epoch 53/100, Training Loss: 0.2983, Validation Loss: 0.2490\n",
      "Epoch 54/100, Training Loss: 0.2925, Validation Loss: 0.2350\n",
      "Epoch 55/100, Training Loss: 0.2876, Validation Loss: 0.2560\n",
      "Epoch 56/100, Training Loss: 0.2959, Validation Loss: 0.2392\n",
      "Epoch 57/100, Training Loss: 0.2887, Validation Loss: 0.2366\n",
      "Epoch 58/100, Training Loss: 0.3006, Validation Loss: 0.2336\n",
      "Epoch 59/100, Training Loss: 0.2979, Validation Loss: 0.2395\n",
      "Epoch 60/100, Training Loss: 0.2928, Validation Loss: 0.2330\n",
      "Epoch 61/100, Training Loss: 0.2807, Validation Loss: 0.2334\n",
      "Epoch 62/100, Training Loss: 0.2877, Validation Loss: 0.2342\n",
      "Epoch 63/100, Training Loss: 0.2962, Validation Loss: 0.2630\n",
      "Epoch 64/100, Training Loss: 0.2926, Validation Loss: 0.2510\n",
      "Epoch 65/100, Training Loss: 0.3040, Validation Loss: 0.2383\n",
      "Epoch 66/100, Training Loss: 0.2824, Validation Loss: 0.2392\n",
      "Epoch 67/100, Training Loss: 0.2899, Validation Loss: 0.2342\n",
      "Epoch 68/100, Training Loss: 0.2839, Validation Loss: 0.2351\n",
      "Epoch 69/100, Training Loss: 0.2967, Validation Loss: 0.2395\n",
      "Epoch 70/100, Training Loss: 0.2845, Validation Loss: 0.2389\n",
      "Epoch 71/100, Training Loss: 0.2852, Validation Loss: 0.2415\n",
      "Epoch 72/100, Training Loss: 0.2823, Validation Loss: 0.2502\n",
      "Epoch 73/100, Training Loss: 0.2881, Validation Loss: 0.2350\n",
      "Epoch 74/100, Training Loss: 0.2867, Validation Loss: 0.2347\n",
      "Epoch 75/100, Training Loss: 0.2895, Validation Loss: 0.2446\n",
      "Epoch 76/100, Training Loss: 0.2846, Validation Loss: 0.2370\n",
      "Epoch 77/100, Training Loss: 0.2800, Validation Loss: 0.2415\n",
      "Epoch 78/100, Training Loss: 0.2820, Validation Loss: 0.2419\n",
      "Epoch 79/100, Training Loss: 0.2870, Validation Loss: 0.2373\n",
      "Epoch 80/100, Training Loss: 0.2792, Validation Loss: 0.2427\n",
      "Epoch 81/100, Training Loss: 0.2797, Validation Loss: 0.2387\n",
      "Epoch 82/100, Training Loss: 0.2798, Validation Loss: 0.2388\n",
      "Epoch 83/100, Training Loss: 0.2873, Validation Loss: 0.2368\n",
      "Epoch 84/100, Training Loss: 0.2788, Validation Loss: 0.2397\n",
      "Epoch 85/100, Training Loss: 0.2906, Validation Loss: 0.2402\n",
      "Epoch 86/100, Training Loss: 0.2953, Validation Loss: 0.2436\n",
      "Epoch 87/100, Training Loss: 0.2850, Validation Loss: 0.2396\n",
      "Epoch 88/100, Training Loss: 0.2801, Validation Loss: 0.2390\n",
      "Epoch 89/100, Training Loss: 0.2708, Validation Loss: 0.2450\n",
      "Epoch 90/100, Training Loss: 0.2796, Validation Loss: 0.2348\n",
      "Epoch 91/100, Training Loss: 0.2728, Validation Loss: 0.2341\n",
      "Epoch 92/100, Training Loss: 0.2725, Validation Loss: 0.2433\n",
      "Epoch 93/100, Training Loss: 0.2646, Validation Loss: 0.2381\n",
      "Epoch 94/100, Training Loss: 0.2738, Validation Loss: 0.2401\n",
      "Epoch 95/100, Training Loss: 0.2795, Validation Loss: 0.2336\n",
      "Epoch 96/100, Training Loss: 0.2796, Validation Loss: 0.2385\n",
      "Epoch 97/100, Training Loss: 0.2749, Validation Loss: 0.2422\n",
      "Epoch 98/100, Training Loss: 0.2821, Validation Loss: 0.2347\n",
      "Epoch 99/100, Training Loss: 0.2680, Validation Loss: 0.2400\n",
      "Epoch 100/100, Training Loss: 0.2801, Validation Loss: 0.2432\n",
      "Epoch 1/100, Training Loss: 0.7202, Validation Loss: 0.5275\n",
      "Epoch 2/100, Training Loss: 0.4425, Validation Loss: 0.4653\n",
      "Epoch 3/100, Training Loss: 0.3839, Validation Loss: 0.4524\n",
      "Epoch 4/100, Training Loss: 0.3502, Validation Loss: 0.4403\n",
      "Epoch 5/100, Training Loss: 0.3339, Validation Loss: 0.4316\n",
      "Epoch 6/100, Training Loss: 0.3198, Validation Loss: 0.4262\n",
      "Epoch 7/100, Training Loss: 0.3161, Validation Loss: 0.4301\n",
      "Epoch 8/100, Training Loss: 0.3148, Validation Loss: 0.4346\n",
      "Epoch 9/100, Training Loss: 0.3076, Validation Loss: 0.4199\n",
      "Epoch 10/100, Training Loss: 0.3089, Validation Loss: 0.4202\n",
      "Epoch 11/100, Training Loss: 0.3071, Validation Loss: 0.4275\n",
      "Epoch 12/100, Training Loss: 0.2996, Validation Loss: 0.4237\n",
      "Epoch 13/100, Training Loss: 0.2931, Validation Loss: 0.4178\n",
      "Epoch 14/100, Training Loss: 0.2916, Validation Loss: 0.4152\n",
      "Epoch 15/100, Training Loss: 0.3035, Validation Loss: 0.4229\n",
      "Epoch 16/100, Training Loss: 0.2939, Validation Loss: 0.4211\n",
      "Epoch 17/100, Training Loss: 0.2925, Validation Loss: 0.4222\n",
      "Epoch 18/100, Training Loss: 0.2832, Validation Loss: 0.4173\n",
      "Epoch 19/100, Training Loss: 0.2902, Validation Loss: 0.4180\n",
      "Epoch 20/100, Training Loss: 0.2947, Validation Loss: 0.4199\n",
      "Epoch 21/100, Training Loss: 0.2934, Validation Loss: 0.4213\n",
      "Epoch 22/100, Training Loss: 0.2997, Validation Loss: 0.4189\n",
      "Epoch 23/100, Training Loss: 0.2932, Validation Loss: 0.4184\n",
      "Epoch 24/100, Training Loss: 0.2915, Validation Loss: 0.4256\n",
      "Epoch 25/100, Training Loss: 0.2834, Validation Loss: 0.4224\n",
      "Epoch 26/100, Training Loss: 0.2880, Validation Loss: 0.4163\n",
      "Epoch 27/100, Training Loss: 0.2896, Validation Loss: 0.4135\n",
      "Epoch 28/100, Training Loss: 0.2964, Validation Loss: 0.4267\n",
      "Epoch 29/100, Training Loss: 0.2983, Validation Loss: 0.4179\n",
      "Epoch 30/100, Training Loss: 0.2921, Validation Loss: 0.4168\n",
      "Epoch 31/100, Training Loss: 0.2950, Validation Loss: 0.4198\n",
      "Epoch 32/100, Training Loss: 0.2850, Validation Loss: 0.4131\n",
      "Epoch 33/100, Training Loss: 0.2842, Validation Loss: 0.4167\n",
      "Epoch 34/100, Training Loss: 0.2854, Validation Loss: 0.4158\n",
      "Epoch 35/100, Training Loss: 0.2823, Validation Loss: 0.4124\n",
      "Epoch 36/100, Training Loss: 0.2872, Validation Loss: 0.4201\n",
      "Epoch 37/100, Training Loss: 0.2902, Validation Loss: 0.4289\n",
      "Epoch 38/100, Training Loss: 0.2960, Validation Loss: 0.4222\n",
      "Epoch 39/100, Training Loss: 0.2953, Validation Loss: 0.4155\n",
      "Epoch 40/100, Training Loss: 0.2809, Validation Loss: 0.4207\n",
      "Epoch 41/100, Training Loss: 0.2903, Validation Loss: 0.4169\n",
      "Epoch 42/100, Training Loss: 0.2907, Validation Loss: 0.4198\n",
      "Epoch 43/100, Training Loss: 0.2924, Validation Loss: 0.4146\n",
      "Epoch 44/100, Training Loss: 0.2830, Validation Loss: 0.4158\n",
      "Epoch 45/100, Training Loss: 0.2822, Validation Loss: 0.4293\n",
      "Epoch 46/100, Training Loss: 0.2854, Validation Loss: 0.4219\n",
      "Epoch 47/100, Training Loss: 0.2765, Validation Loss: 0.4227\n",
      "Epoch 48/100, Training Loss: 0.2803, Validation Loss: 0.4186\n",
      "Epoch 49/100, Training Loss: 0.2798, Validation Loss: 0.4250\n",
      "Epoch 50/100, Training Loss: 0.2918, Validation Loss: 0.4178\n",
      "Epoch 51/100, Training Loss: 0.2802, Validation Loss: 0.4185\n",
      "Epoch 52/100, Training Loss: 0.2754, Validation Loss: 0.4198\n",
      "Epoch 53/100, Training Loss: 0.2829, Validation Loss: 0.4322\n",
      "Epoch 54/100, Training Loss: 0.2761, Validation Loss: 0.4193\n",
      "Epoch 55/100, Training Loss: 0.2734, Validation Loss: 0.4196\n",
      "Epoch 56/100, Training Loss: 0.2823, Validation Loss: 0.4156\n",
      "Epoch 57/100, Training Loss: 0.2788, Validation Loss: 0.4229\n",
      "Epoch 58/100, Training Loss: 0.2868, Validation Loss: 0.4181\n",
      "Epoch 59/100, Training Loss: 0.2756, Validation Loss: 0.4291\n",
      "Epoch 60/100, Training Loss: 0.2940, Validation Loss: 0.4188\n",
      "Epoch 61/100, Training Loss: 0.2787, Validation Loss: 0.4129\n",
      "Epoch 62/100, Training Loss: 0.2770, Validation Loss: 0.4154\n",
      "Epoch 63/100, Training Loss: 0.2764, Validation Loss: 0.4169\n",
      "Epoch 64/100, Training Loss: 0.2855, Validation Loss: 0.4172\n",
      "Epoch 65/100, Training Loss: 0.2857, Validation Loss: 0.4214\n",
      "Epoch 66/100, Training Loss: 0.2930, Validation Loss: 0.4258\n",
      "Epoch 67/100, Training Loss: 0.2855, Validation Loss: 0.4265\n",
      "Epoch 68/100, Training Loss: 0.2754, Validation Loss: 0.4277\n",
      "Epoch 69/100, Training Loss: 0.2896, Validation Loss: 0.4187\n",
      "Epoch 70/100, Training Loss: 0.2864, Validation Loss: 0.4204\n",
      "Epoch 71/100, Training Loss: 0.2770, Validation Loss: 0.4232\n",
      "Epoch 72/100, Training Loss: 0.2778, Validation Loss: 0.4289\n",
      "Epoch 73/100, Training Loss: 0.2806, Validation Loss: 0.4254\n",
      "Epoch 74/100, Training Loss: 0.2791, Validation Loss: 0.4241\n",
      "Epoch 75/100, Training Loss: 0.2876, Validation Loss: 0.4179\n",
      "Epoch 76/100, Training Loss: 0.2727, Validation Loss: 0.4210\n",
      "Epoch 77/100, Training Loss: 0.2859, Validation Loss: 0.4290\n",
      "Epoch 78/100, Training Loss: 0.2699, Validation Loss: 0.4181\n",
      "Epoch 79/100, Training Loss: 0.2788, Validation Loss: 0.4228\n",
      "Epoch 80/100, Training Loss: 0.2863, Validation Loss: 0.4205\n",
      "Epoch 81/100, Training Loss: 0.2764, Validation Loss: 0.4163\n",
      "Epoch 82/100, Training Loss: 0.2710, Validation Loss: 0.4238\n",
      "Epoch 83/100, Training Loss: 0.2736, Validation Loss: 0.4191\n",
      "Epoch 84/100, Training Loss: 0.2830, Validation Loss: 0.4311\n",
      "Epoch 85/100, Training Loss: 0.2733, Validation Loss: 0.4260\n",
      "Epoch 86/100, Training Loss: 0.2870, Validation Loss: 0.4236\n",
      "Epoch 87/100, Training Loss: 0.2832, Validation Loss: 0.4179\n",
      "Epoch 88/100, Training Loss: 0.2792, Validation Loss: 0.4295\n",
      "Epoch 89/100, Training Loss: 0.2802, Validation Loss: 0.4197\n",
      "Epoch 90/100, Training Loss: 0.2749, Validation Loss: 0.4250\n",
      "Epoch 91/100, Training Loss: 0.2754, Validation Loss: 0.4296\n",
      "Epoch 92/100, Training Loss: 0.2793, Validation Loss: 0.4167\n",
      "Epoch 93/100, Training Loss: 0.2817, Validation Loss: 0.4215\n",
      "Epoch 94/100, Training Loss: 0.2723, Validation Loss: 0.4235\n",
      "Epoch 95/100, Training Loss: 0.2781, Validation Loss: 0.4281\n",
      "Epoch 96/100, Training Loss: 0.2736, Validation Loss: 0.4204\n",
      "Epoch 97/100, Training Loss: 0.2733, Validation Loss: 0.4194\n",
      "Epoch 98/100, Training Loss: 0.2716, Validation Loss: 0.4167\n",
      "Epoch 99/100, Training Loss: 0.2783, Validation Loss: 0.4254\n",
      "Epoch 100/100, Training Loss: 0.2713, Validation Loss: 0.4269\n",
      "Epoch 1/100, Training Loss: 0.6607, Validation Loss: 0.4975\n",
      "Epoch 2/100, Training Loss: 0.3576, Validation Loss: 0.4499\n",
      "Epoch 3/100, Training Loss: 0.3204, Validation Loss: 0.4353\n",
      "Epoch 4/100, Training Loss: 0.3004, Validation Loss: 0.4245\n",
      "Epoch 5/100, Training Loss: 0.2839, Validation Loss: 0.4227\n",
      "Epoch 6/100, Training Loss: 0.2713, Validation Loss: 0.4147\n",
      "Epoch 7/100, Training Loss: 0.2767, Validation Loss: 0.4213\n",
      "Epoch 8/100, Training Loss: 0.2758, Validation Loss: 0.4176\n",
      "Epoch 9/100, Training Loss: 0.2686, Validation Loss: 0.4157\n",
      "Epoch 10/100, Training Loss: 0.2595, Validation Loss: 0.4184\n",
      "Epoch 11/100, Training Loss: 0.2640, Validation Loss: 0.4179\n",
      "Epoch 12/100, Training Loss: 0.2703, Validation Loss: 0.4201\n",
      "Epoch 13/100, Training Loss: 0.2628, Validation Loss: 0.4198\n",
      "Epoch 14/100, Training Loss: 0.2581, Validation Loss: 0.4128\n",
      "Epoch 15/100, Training Loss: 0.2636, Validation Loss: 0.4227\n",
      "Epoch 16/100, Training Loss: 0.2569, Validation Loss: 0.4187\n",
      "Epoch 17/100, Training Loss: 0.2564, Validation Loss: 0.4200\n",
      "Epoch 18/100, Training Loss: 0.2457, Validation Loss: 0.4192\n",
      "Epoch 19/100, Training Loss: 0.2514, Validation Loss: 0.4173\n",
      "Epoch 20/100, Training Loss: 0.2525, Validation Loss: 0.4166\n",
      "Epoch 21/100, Training Loss: 0.2585, Validation Loss: 0.4136\n",
      "Epoch 22/100, Training Loss: 0.2556, Validation Loss: 0.4177\n",
      "Epoch 23/100, Training Loss: 0.2540, Validation Loss: 0.4193\n",
      "Epoch 24/100, Training Loss: 0.2485, Validation Loss: 0.4167\n",
      "Epoch 25/100, Training Loss: 0.2515, Validation Loss: 0.4445\n",
      "Epoch 26/100, Training Loss: 0.2530, Validation Loss: 0.4197\n",
      "Epoch 27/100, Training Loss: 0.2527, Validation Loss: 0.4135\n",
      "Epoch 28/100, Training Loss: 0.2525, Validation Loss: 0.4264\n",
      "Epoch 29/100, Training Loss: 0.2541, Validation Loss: 0.4168\n",
      "Epoch 30/100, Training Loss: 0.2521, Validation Loss: 0.4135\n",
      "Epoch 31/100, Training Loss: 0.2590, Validation Loss: 0.4158\n",
      "Epoch 32/100, Training Loss: 0.2463, Validation Loss: 0.4177\n",
      "Epoch 33/100, Training Loss: 0.2484, Validation Loss: 0.4184\n",
      "Epoch 34/100, Training Loss: 0.2439, Validation Loss: 0.4300\n",
      "Epoch 35/100, Training Loss: 0.2537, Validation Loss: 0.4219\n",
      "Epoch 36/100, Training Loss: 0.2440, Validation Loss: 0.4231\n",
      "Epoch 37/100, Training Loss: 0.2543, Validation Loss: 0.4227\n",
      "Epoch 38/100, Training Loss: 0.2524, Validation Loss: 0.4268\n",
      "Epoch 39/100, Training Loss: 0.2498, Validation Loss: 0.4205\n",
      "Epoch 40/100, Training Loss: 0.2418, Validation Loss: 0.4205\n",
      "Epoch 41/100, Training Loss: 0.2459, Validation Loss: 0.4191\n",
      "Epoch 42/100, Training Loss: 0.2447, Validation Loss: 0.4225\n",
      "Epoch 43/100, Training Loss: 0.2494, Validation Loss: 0.4159\n",
      "Epoch 44/100, Training Loss: 0.2412, Validation Loss: 0.4168\n",
      "Epoch 45/100, Training Loss: 0.2297, Validation Loss: 0.4206\n",
      "Epoch 46/100, Training Loss: 0.2486, Validation Loss: 0.4182\n",
      "Epoch 47/100, Training Loss: 0.2391, Validation Loss: 0.4222\n",
      "Epoch 48/100, Training Loss: 0.2386, Validation Loss: 0.4208\n",
      "Epoch 49/100, Training Loss: 0.2395, Validation Loss: 0.4240\n",
      "Epoch 50/100, Training Loss: 0.2397, Validation Loss: 0.4209\n",
      "Epoch 51/100, Training Loss: 0.2411, Validation Loss: 0.4224\n",
      "Epoch 52/100, Training Loss: 0.2350, Validation Loss: 0.4235\n",
      "Epoch 53/100, Training Loss: 0.2419, Validation Loss: 0.4227\n",
      "Epoch 54/100, Training Loss: 0.2361, Validation Loss: 0.4215\n",
      "Epoch 55/100, Training Loss: 0.2374, Validation Loss: 0.4190\n",
      "Epoch 56/100, Training Loss: 0.2346, Validation Loss: 0.4246\n",
      "Epoch 57/100, Training Loss: 0.2338, Validation Loss: 0.4251\n",
      "Epoch 58/100, Training Loss: 0.2423, Validation Loss: 0.4231\n",
      "Epoch 59/100, Training Loss: 0.2323, Validation Loss: 0.4186\n",
      "Epoch 60/100, Training Loss: 0.2391, Validation Loss: 0.4207\n",
      "Epoch 61/100, Training Loss: 0.2417, Validation Loss: 0.4177\n",
      "Epoch 62/100, Training Loss: 0.2331, Validation Loss: 0.4192\n",
      "Epoch 63/100, Training Loss: 0.2379, Validation Loss: 0.4172\n",
      "Epoch 64/100, Training Loss: 0.2337, Validation Loss: 0.4218\n",
      "Epoch 65/100, Training Loss: 0.2391, Validation Loss: 0.4183\n",
      "Epoch 66/100, Training Loss: 0.2493, Validation Loss: 0.4252\n",
      "Epoch 67/100, Training Loss: 0.2317, Validation Loss: 0.4253\n",
      "Epoch 68/100, Training Loss: 0.2323, Validation Loss: 0.4341\n",
      "Epoch 69/100, Training Loss: 0.2411, Validation Loss: 0.4196\n",
      "Epoch 70/100, Training Loss: 0.2393, Validation Loss: 0.4237\n",
      "Epoch 71/100, Training Loss: 0.2376, Validation Loss: 0.4260\n",
      "Epoch 72/100, Training Loss: 0.2353, Validation Loss: 0.4247\n",
      "Epoch 73/100, Training Loss: 0.2334, Validation Loss: 0.4263\n",
      "Epoch 74/100, Training Loss: 0.2370, Validation Loss: 0.4261\n",
      "Epoch 75/100, Training Loss: 0.2401, Validation Loss: 0.4199\n",
      "Epoch 76/100, Training Loss: 0.2304, Validation Loss: 0.4233\n",
      "Epoch 77/100, Training Loss: 0.2363, Validation Loss: 0.4274\n",
      "Epoch 78/100, Training Loss: 0.2349, Validation Loss: 0.4222\n",
      "Epoch 79/100, Training Loss: 0.2378, Validation Loss: 0.4252\n",
      "Epoch 80/100, Training Loss: 0.2328, Validation Loss: 0.4221\n",
      "Epoch 81/100, Training Loss: 0.2303, Validation Loss: 0.4220\n",
      "Epoch 82/100, Training Loss: 0.2289, Validation Loss: 0.4286\n",
      "Epoch 83/100, Training Loss: 0.2292, Validation Loss: 0.4212\n",
      "Epoch 84/100, Training Loss: 0.2318, Validation Loss: 0.4268\n",
      "Epoch 85/100, Training Loss: 0.2246, Validation Loss: 0.4247\n",
      "Epoch 86/100, Training Loss: 0.2274, Validation Loss: 0.4324\n",
      "Epoch 87/100, Training Loss: 0.2312, Validation Loss: 0.4262\n",
      "Epoch 88/100, Training Loss: 0.2325, Validation Loss: 0.4334\n",
      "Epoch 89/100, Training Loss: 0.2302, Validation Loss: 0.4255\n",
      "Epoch 90/100, Training Loss: 0.2320, Validation Loss: 0.4256\n",
      "Epoch 91/100, Training Loss: 0.2294, Validation Loss: 0.4298\n",
      "Epoch 92/100, Training Loss: 0.2296, Validation Loss: 0.4270\n",
      "Epoch 93/100, Training Loss: 0.2261, Validation Loss: 0.4294\n",
      "Epoch 94/100, Training Loss: 0.2249, Validation Loss: 0.4250\n",
      "Epoch 95/100, Training Loss: 0.2198, Validation Loss: 0.4270\n",
      "Epoch 96/100, Training Loss: 0.2212, Validation Loss: 0.4294\n",
      "Epoch 97/100, Training Loss: 0.2236, Validation Loss: 0.4197\n",
      "Epoch 98/100, Training Loss: 0.2274, Validation Loss: 0.4250\n",
      "Epoch 99/100, Training Loss: 0.2265, Validation Loss: 0.4277\n",
      "Epoch 100/100, Training Loss: 0.2209, Validation Loss: 0.4298\n",
      "Epoch 1/100, Training Loss: 1.2812, Validation Loss: 1.0986\n",
      "Epoch 2/100, Training Loss: 1.1161, Validation Loss: 1.0202\n",
      "Epoch 3/100, Training Loss: 0.9670, Validation Loss: 0.9245\n",
      "Epoch 4/100, Training Loss: 0.8471, Validation Loss: 0.8245\n",
      "Epoch 5/100, Training Loss: 0.7549, Validation Loss: 0.7343\n",
      "Epoch 6/100, Training Loss: 0.6929, Validation Loss: 0.6576\n",
      "Epoch 7/100, Training Loss: 0.6312, Validation Loss: 0.6021\n",
      "Epoch 8/100, Training Loss: 0.5964, Validation Loss: 0.5640\n",
      "Epoch 9/100, Training Loss: 0.5733, Validation Loss: 0.5364\n",
      "Epoch 10/100, Training Loss: 0.5687, Validation Loss: 0.5172\n",
      "Epoch 11/100, Training Loss: 0.5075, Validation Loss: 0.5027\n",
      "Epoch 12/100, Training Loss: 0.5028, Validation Loss: 0.4925\n",
      "Epoch 13/100, Training Loss: 0.4929, Validation Loss: 0.4819\n",
      "Epoch 14/100, Training Loss: 0.4709, Validation Loss: 0.4737\n",
      "Epoch 15/100, Training Loss: 0.4686, Validation Loss: 0.4677\n",
      "Epoch 16/100, Training Loss: 0.4530, Validation Loss: 0.4640\n",
      "Epoch 17/100, Training Loss: 0.4348, Validation Loss: 0.4603\n",
      "Epoch 18/100, Training Loss: 0.4366, Validation Loss: 0.4572\n",
      "Epoch 19/100, Training Loss: 0.4073, Validation Loss: 0.4542\n",
      "Epoch 20/100, Training Loss: 0.4113, Validation Loss: 0.4531\n",
      "Epoch 21/100, Training Loss: 0.4148, Validation Loss: 0.4521\n",
      "Epoch 22/100, Training Loss: 0.3993, Validation Loss: 0.4495\n",
      "Epoch 23/100, Training Loss: 0.3957, Validation Loss: 0.4471\n",
      "Epoch 24/100, Training Loss: 0.3814, Validation Loss: 0.4449\n",
      "Epoch 25/100, Training Loss: 0.3896, Validation Loss: 0.4424\n",
      "Epoch 26/100, Training Loss: 0.3791, Validation Loss: 0.4403\n",
      "Epoch 27/100, Training Loss: 0.3768, Validation Loss: 0.4384\n",
      "Epoch 28/100, Training Loss: 0.3697, Validation Loss: 0.4367\n",
      "Epoch 29/100, Training Loss: 0.3629, Validation Loss: 0.4363\n",
      "Epoch 30/100, Training Loss: 0.3613, Validation Loss: 0.4356\n",
      "Epoch 31/100, Training Loss: 0.3592, Validation Loss: 0.4339\n",
      "Epoch 32/100, Training Loss: 0.3514, Validation Loss: 0.4342\n",
      "Epoch 33/100, Training Loss: 0.3487, Validation Loss: 0.4340\n",
      "Epoch 34/100, Training Loss: 0.3534, Validation Loss: 0.4340\n",
      "Epoch 35/100, Training Loss: 0.3503, Validation Loss: 0.4334\n",
      "Epoch 36/100, Training Loss: 0.3349, Validation Loss: 0.4329\n",
      "Epoch 37/100, Training Loss: 0.3392, Validation Loss: 0.4324\n",
      "Epoch 38/100, Training Loss: 0.3402, Validation Loss: 0.4314\n",
      "Epoch 39/100, Training Loss: 0.3393, Validation Loss: 0.4302\n",
      "Epoch 40/100, Training Loss: 0.3427, Validation Loss: 0.4291\n",
      "Epoch 41/100, Training Loss: 0.3412, Validation Loss: 0.4285\n",
      "Epoch 42/100, Training Loss: 0.3243, Validation Loss: 0.4286\n",
      "Epoch 43/100, Training Loss: 0.3449, Validation Loss: 0.4278\n",
      "Epoch 44/100, Training Loss: 0.3374, Validation Loss: 0.4270\n",
      "Epoch 45/100, Training Loss: 0.3253, Validation Loss: 0.4260\n",
      "Epoch 46/100, Training Loss: 0.3255, Validation Loss: 0.4251\n",
      "Epoch 47/100, Training Loss: 0.3265, Validation Loss: 0.4240\n",
      "Epoch 48/100, Training Loss: 0.3235, Validation Loss: 0.4241\n",
      "Epoch 49/100, Training Loss: 0.3237, Validation Loss: 0.4245\n",
      "Epoch 50/100, Training Loss: 0.3250, Validation Loss: 0.4245\n",
      "Epoch 51/100, Training Loss: 0.3336, Validation Loss: 0.4246\n",
      "Epoch 52/100, Training Loss: 0.3161, Validation Loss: 0.4248\n",
      "Epoch 53/100, Training Loss: 0.3243, Validation Loss: 0.4240\n",
      "Epoch 54/100, Training Loss: 0.3215, Validation Loss: 0.4239\n",
      "Epoch 55/100, Training Loss: 0.3128, Validation Loss: 0.4229\n",
      "Epoch 56/100, Training Loss: 0.3116, Validation Loss: 0.4228\n",
      "Epoch 57/100, Training Loss: 0.3152, Validation Loss: 0.4227\n",
      "Epoch 58/100, Training Loss: 0.3160, Validation Loss: 0.4225\n",
      "Epoch 59/100, Training Loss: 0.3201, Validation Loss: 0.4227\n",
      "Epoch 60/100, Training Loss: 0.3181, Validation Loss: 0.4233\n",
      "Epoch 61/100, Training Loss: 0.3088, Validation Loss: 0.4235\n",
      "Epoch 62/100, Training Loss: 0.3132, Validation Loss: 0.4232\n",
      "Epoch 63/100, Training Loss: 0.3139, Validation Loss: 0.4233\n",
      "Epoch 64/100, Training Loss: 0.3044, Validation Loss: 0.4232\n",
      "Epoch 65/100, Training Loss: 0.3023, Validation Loss: 0.4231\n",
      "Epoch 66/100, Training Loss: 0.3087, Validation Loss: 0.4226\n",
      "Epoch 67/100, Training Loss: 0.3114, Validation Loss: 0.4225\n",
      "Epoch 68/100, Training Loss: 0.3144, Validation Loss: 0.4219\n",
      "Epoch 69/100, Training Loss: 0.3026, Validation Loss: 0.4216\n",
      "Epoch 70/100, Training Loss: 0.2987, Validation Loss: 0.4219\n",
      "Epoch 71/100, Training Loss: 0.3136, Validation Loss: 0.4225\n",
      "Epoch 72/100, Training Loss: 0.3035, Validation Loss: 0.4231\n",
      "Epoch 73/100, Training Loss: 0.3092, Validation Loss: 0.4238\n",
      "Epoch 74/100, Training Loss: 0.3188, Validation Loss: 0.4248\n",
      "Epoch 75/100, Training Loss: 0.3059, Validation Loss: 0.4236\n",
      "Epoch 76/100, Training Loss: 0.2992, Validation Loss: 0.4228\n",
      "Epoch 77/100, Training Loss: 0.3038, Validation Loss: 0.4220\n",
      "Epoch 78/100, Training Loss: 0.3037, Validation Loss: 0.4217\n",
      "Epoch 79/100, Training Loss: 0.3071, Validation Loss: 0.4216\n",
      "Epoch 80/100, Training Loss: 0.3010, Validation Loss: 0.4219\n",
      "Epoch 81/100, Training Loss: 0.2986, Validation Loss: 0.4221\n",
      "Epoch 82/100, Training Loss: 0.2943, Validation Loss: 0.4222\n",
      "Epoch 83/100, Training Loss: 0.2983, Validation Loss: 0.4227\n",
      "Epoch 84/100, Training Loss: 0.2982, Validation Loss: 0.4227\n",
      "Epoch 85/100, Training Loss: 0.3082, Validation Loss: 0.4232\n",
      "Epoch 86/100, Training Loss: 0.2893, Validation Loss: 0.4230\n",
      "Epoch 87/100, Training Loss: 0.3050, Validation Loss: 0.4228\n",
      "Epoch 88/100, Training Loss: 0.2985, Validation Loss: 0.4222\n",
      "Epoch 89/100, Training Loss: 0.3115, Validation Loss: 0.4215\n",
      "Epoch 90/100, Training Loss: 0.3014, Validation Loss: 0.4214\n",
      "Epoch 91/100, Training Loss: 0.3067, Validation Loss: 0.4220\n",
      "Epoch 92/100, Training Loss: 0.3020, Validation Loss: 0.4219\n",
      "Epoch 93/100, Training Loss: 0.3021, Validation Loss: 0.4222\n",
      "Epoch 94/100, Training Loss: 0.2871, Validation Loss: 0.4222\n",
      "Epoch 95/100, Training Loss: 0.2958, Validation Loss: 0.4219\n",
      "Epoch 96/100, Training Loss: 0.2929, Validation Loss: 0.4219\n",
      "Epoch 97/100, Training Loss: 0.2960, Validation Loss: 0.4220\n",
      "Epoch 98/100, Training Loss: 0.3004, Validation Loss: 0.4223\n",
      "Epoch 99/100, Training Loss: 0.2968, Validation Loss: 0.4225\n",
      "Epoch 100/100, Training Loss: 0.2970, Validation Loss: 0.4226\n",
      "Epoch 1/100, Training Loss: 1.0128, Validation Loss: 1.0989\n",
      "Epoch 2/100, Training Loss: 0.9622, Validation Loss: 1.0486\n",
      "Epoch 3/100, Training Loss: 0.9165, Validation Loss: 0.9960\n",
      "Epoch 4/100, Training Loss: 0.8564, Validation Loss: 0.9378\n",
      "Epoch 5/100, Training Loss: 0.8054, Validation Loss: 0.8735\n",
      "Epoch 6/100, Training Loss: 0.7461, Validation Loss: 0.8028\n",
      "Epoch 7/100, Training Loss: 0.6793, Validation Loss: 0.7303\n",
      "Epoch 8/100, Training Loss: 0.6165, Validation Loss: 0.6608\n",
      "Epoch 9/100, Training Loss: 0.5791, Validation Loss: 0.5997\n",
      "Epoch 10/100, Training Loss: 0.5405, Validation Loss: 0.5523\n",
      "Epoch 11/100, Training Loss: 0.4772, Validation Loss: 0.5175\n",
      "Epoch 12/100, Training Loss: 0.4712, Validation Loss: 0.4923\n",
      "Epoch 13/100, Training Loss: 0.4509, Validation Loss: 0.4739\n",
      "Epoch 14/100, Training Loss: 0.4336, Validation Loss: 0.4612\n",
      "Epoch 15/100, Training Loss: 0.4227, Validation Loss: 0.4534\n",
      "Epoch 16/100, Training Loss: 0.4095, Validation Loss: 0.4506\n",
      "Epoch 17/100, Training Loss: 0.3962, Validation Loss: 0.4482\n",
      "Epoch 18/100, Training Loss: 0.3933, Validation Loss: 0.4456\n",
      "Epoch 19/100, Training Loss: 0.3689, Validation Loss: 0.4412\n",
      "Epoch 20/100, Training Loss: 0.3716, Validation Loss: 0.4387\n",
      "Epoch 21/100, Training Loss: 0.3739, Validation Loss: 0.4373\n",
      "Epoch 22/100, Training Loss: 0.3578, Validation Loss: 0.4353\n",
      "Epoch 23/100, Training Loss: 0.3566, Validation Loss: 0.4332\n",
      "Epoch 24/100, Training Loss: 0.3486, Validation Loss: 0.4309\n",
      "Epoch 25/100, Training Loss: 0.3516, Validation Loss: 0.4297\n",
      "Epoch 26/100, Training Loss: 0.3436, Validation Loss: 0.4294\n",
      "Epoch 27/100, Training Loss: 0.3462, Validation Loss: 0.4291\n",
      "Epoch 28/100, Training Loss: 0.3411, Validation Loss: 0.4288\n",
      "Epoch 29/100, Training Loss: 0.3313, Validation Loss: 0.4280\n",
      "Epoch 30/100, Training Loss: 0.3336, Validation Loss: 0.4280\n",
      "Epoch 31/100, Training Loss: 0.3340, Validation Loss: 0.4268\n",
      "Epoch 32/100, Training Loss: 0.3249, Validation Loss: 0.4266\n",
      "Epoch 33/100, Training Loss: 0.3237, Validation Loss: 0.4271\n",
      "Epoch 34/100, Training Loss: 0.3280, Validation Loss: 0.4276\n",
      "Epoch 35/100, Training Loss: 0.3256, Validation Loss: 0.4269\n",
      "Epoch 36/100, Training Loss: 0.3129, Validation Loss: 0.4266\n",
      "Epoch 37/100, Training Loss: 0.3184, Validation Loss: 0.4262\n",
      "Epoch 38/100, Training Loss: 0.3160, Validation Loss: 0.4259\n",
      "Epoch 39/100, Training Loss: 0.3185, Validation Loss: 0.4246\n",
      "Epoch 40/100, Training Loss: 0.3214, Validation Loss: 0.4240\n",
      "Epoch 41/100, Training Loss: 0.3217, Validation Loss: 0.4246\n",
      "Epoch 42/100, Training Loss: 0.3047, Validation Loss: 0.4254\n",
      "Epoch 43/100, Training Loss: 0.3198, Validation Loss: 0.4244\n",
      "Epoch 44/100, Training Loss: 0.3135, Validation Loss: 0.4228\n",
      "Epoch 45/100, Training Loss: 0.3085, Validation Loss: 0.4217\n",
      "Epoch 46/100, Training Loss: 0.3049, Validation Loss: 0.4213\n",
      "Epoch 47/100, Training Loss: 0.3100, Validation Loss: 0.4213\n",
      "Epoch 48/100, Training Loss: 0.3092, Validation Loss: 0.4222\n",
      "Epoch 49/100, Training Loss: 0.3083, Validation Loss: 0.4232\n",
      "Epoch 50/100, Training Loss: 0.3094, Validation Loss: 0.4228\n",
      "Epoch 51/100, Training Loss: 0.3163, Validation Loss: 0.4222\n",
      "Epoch 52/100, Training Loss: 0.3024, Validation Loss: 0.4215\n",
      "Epoch 53/100, Training Loss: 0.3049, Validation Loss: 0.4206\n",
      "Epoch 54/100, Training Loss: 0.3043, Validation Loss: 0.4203\n",
      "Epoch 55/100, Training Loss: 0.2985, Validation Loss: 0.4207\n",
      "Epoch 56/100, Training Loss: 0.2933, Validation Loss: 0.4214\n",
      "Epoch 57/100, Training Loss: 0.2996, Validation Loss: 0.4223\n",
      "Epoch 58/100, Training Loss: 0.3045, Validation Loss: 0.4225\n",
      "Epoch 59/100, Training Loss: 0.3050, Validation Loss: 0.4221\n",
      "Epoch 60/100, Training Loss: 0.3030, Validation Loss: 0.4221\n",
      "Epoch 61/100, Training Loss: 0.2926, Validation Loss: 0.4222\n",
      "Epoch 62/100, Training Loss: 0.2981, Validation Loss: 0.4219\n",
      "Epoch 63/100, Training Loss: 0.3007, Validation Loss: 0.4222\n",
      "Epoch 64/100, Training Loss: 0.2918, Validation Loss: 0.4217\n",
      "Epoch 65/100, Training Loss: 0.2922, Validation Loss: 0.4215\n",
      "Epoch 66/100, Training Loss: 0.2972, Validation Loss: 0.4215\n",
      "Epoch 67/100, Training Loss: 0.2993, Validation Loss: 0.4210\n",
      "Epoch 68/100, Training Loss: 0.2996, Validation Loss: 0.4202\n",
      "Epoch 69/100, Training Loss: 0.2900, Validation Loss: 0.4208\n",
      "Epoch 70/100, Training Loss: 0.2869, Validation Loss: 0.4219\n",
      "Epoch 71/100, Training Loss: 0.2983, Validation Loss: 0.4224\n",
      "Epoch 72/100, Training Loss: 0.2906, Validation Loss: 0.4220\n",
      "Epoch 73/100, Training Loss: 0.2969, Validation Loss: 0.4219\n",
      "Epoch 74/100, Training Loss: 0.3052, Validation Loss: 0.4226\n",
      "Epoch 75/100, Training Loss: 0.2951, Validation Loss: 0.4217\n",
      "Epoch 76/100, Training Loss: 0.2866, Validation Loss: 0.4206\n",
      "Epoch 77/100, Training Loss: 0.2922, Validation Loss: 0.4201\n",
      "Epoch 78/100, Training Loss: 0.2933, Validation Loss: 0.4205\n",
      "Epoch 79/100, Training Loss: 0.2949, Validation Loss: 0.4213\n",
      "Epoch 80/100, Training Loss: 0.2887, Validation Loss: 0.4214\n",
      "Epoch 81/100, Training Loss: 0.2847, Validation Loss: 0.4212\n",
      "Epoch 82/100, Training Loss: 0.2816, Validation Loss: 0.4206\n",
      "Epoch 83/100, Training Loss: 0.2891, Validation Loss: 0.4206\n",
      "Epoch 84/100, Training Loss: 0.2876, Validation Loss: 0.4211\n",
      "Epoch 85/100, Training Loss: 0.3015, Validation Loss: 0.4222\n",
      "Epoch 86/100, Training Loss: 0.2802, Validation Loss: 0.4219\n",
      "Epoch 87/100, Training Loss: 0.2948, Validation Loss: 0.4216\n",
      "Epoch 88/100, Training Loss: 0.2875, Validation Loss: 0.4206\n",
      "Epoch 89/100, Training Loss: 0.2977, Validation Loss: 0.4198\n",
      "Epoch 90/100, Training Loss: 0.2904, Validation Loss: 0.4199\n",
      "Epoch 91/100, Training Loss: 0.2966, Validation Loss: 0.4219\n",
      "Epoch 92/100, Training Loss: 0.2905, Validation Loss: 0.4222\n",
      "Epoch 93/100, Training Loss: 0.2913, Validation Loss: 0.4210\n",
      "Epoch 94/100, Training Loss: 0.2792, Validation Loss: 0.4207\n",
      "Epoch 95/100, Training Loss: 0.2879, Validation Loss: 0.4207\n",
      "Epoch 96/100, Training Loss: 0.2831, Validation Loss: 0.4219\n",
      "Epoch 97/100, Training Loss: 0.2856, Validation Loss: 0.4222\n",
      "Epoch 98/100, Training Loss: 0.2866, Validation Loss: 0.4215\n",
      "Epoch 99/100, Training Loss: 0.2875, Validation Loss: 0.4210\n",
      "Epoch 100/100, Training Loss: 0.2887, Validation Loss: 0.4208\n",
      "Epoch 1/100, Training Loss: 1.2057, Validation Loss: 0.9761\n",
      "Epoch 2/100, Training Loss: 1.0373, Validation Loss: 0.8939\n",
      "Epoch 3/100, Training Loss: 0.9138, Validation Loss: 0.7938\n",
      "Epoch 4/100, Training Loss: 0.8147, Validation Loss: 0.6916\n",
      "Epoch 5/100, Training Loss: 0.7523, Validation Loss: 0.5962\n",
      "Epoch 6/100, Training Loss: 0.6640, Validation Loss: 0.5158\n",
      "Epoch 7/100, Training Loss: 0.6406, Validation Loss: 0.4531\n",
      "Epoch 8/100, Training Loss: 0.6123, Validation Loss: 0.4074\n",
      "Epoch 9/100, Training Loss: 0.5831, Validation Loss: 0.3751\n",
      "Epoch 10/100, Training Loss: 0.5691, Validation Loss: 0.3530\n",
      "Epoch 11/100, Training Loss: 0.5460, Validation Loss: 0.3356\n",
      "Epoch 12/100, Training Loss: 0.5163, Validation Loss: 0.3227\n",
      "Epoch 13/100, Training Loss: 0.4906, Validation Loss: 0.3117\n",
      "Epoch 14/100, Training Loss: 0.4771, Validation Loss: 0.3037\n",
      "Epoch 15/100, Training Loss: 0.4604, Validation Loss: 0.2973\n",
      "Epoch 16/100, Training Loss: 0.4577, Validation Loss: 0.2910\n",
      "Epoch 17/100, Training Loss: 0.4636, Validation Loss: 0.2854\n",
      "Epoch 18/100, Training Loss: 0.4444, Validation Loss: 0.2804\n",
      "Epoch 19/100, Training Loss: 0.4264, Validation Loss: 0.2757\n",
      "Epoch 20/100, Training Loss: 0.4141, Validation Loss: 0.2721\n",
      "Epoch 21/100, Training Loss: 0.4224, Validation Loss: 0.2692\n",
      "Epoch 22/100, Training Loss: 0.4025, Validation Loss: 0.2664\n",
      "Epoch 23/100, Training Loss: 0.3912, Validation Loss: 0.2638\n",
      "Epoch 24/100, Training Loss: 0.4116, Validation Loss: 0.2622\n",
      "Epoch 25/100, Training Loss: 0.3885, Validation Loss: 0.2605\n",
      "Epoch 26/100, Training Loss: 0.3938, Validation Loss: 0.2584\n",
      "Epoch 27/100, Training Loss: 0.3899, Validation Loss: 0.2573\n",
      "Epoch 28/100, Training Loss: 0.3760, Validation Loss: 0.2558\n",
      "Epoch 29/100, Training Loss: 0.3825, Validation Loss: 0.2557\n",
      "Epoch 30/100, Training Loss: 0.3691, Validation Loss: 0.2556\n",
      "Epoch 31/100, Training Loss: 0.3747, Validation Loss: 0.2552\n",
      "Epoch 32/100, Training Loss: 0.3735, Validation Loss: 0.2537\n",
      "Epoch 33/100, Training Loss: 0.3743, Validation Loss: 0.2526\n",
      "Epoch 34/100, Training Loss: 0.3735, Validation Loss: 0.2522\n",
      "Epoch 35/100, Training Loss: 0.3639, Validation Loss: 0.2507\n",
      "Epoch 36/100, Training Loss: 0.3642, Validation Loss: 0.2482\n",
      "Epoch 37/100, Training Loss: 0.3617, Validation Loss: 0.2465\n",
      "Epoch 38/100, Training Loss: 0.3596, Validation Loss: 0.2454\n",
      "Epoch 39/100, Training Loss: 0.3618, Validation Loss: 0.2451\n",
      "Epoch 40/100, Training Loss: 0.3484, Validation Loss: 0.2457\n",
      "Epoch 41/100, Training Loss: 0.3507, Validation Loss: 0.2463\n",
      "Epoch 42/100, Training Loss: 0.3556, Validation Loss: 0.2465\n",
      "Epoch 43/100, Training Loss: 0.3483, Validation Loss: 0.2451\n",
      "Epoch 44/100, Training Loss: 0.3526, Validation Loss: 0.2438\n",
      "Epoch 45/100, Training Loss: 0.3426, Validation Loss: 0.2423\n",
      "Epoch 46/100, Training Loss: 0.3447, Validation Loss: 0.2411\n",
      "Epoch 47/100, Training Loss: 0.3361, Validation Loss: 0.2400\n",
      "Epoch 48/100, Training Loss: 0.3487, Validation Loss: 0.2393\n",
      "Epoch 49/100, Training Loss: 0.3442, Validation Loss: 0.2399\n",
      "Epoch 50/100, Training Loss: 0.3371, Validation Loss: 0.2402\n",
      "Epoch 51/100, Training Loss: 0.3404, Validation Loss: 0.2399\n",
      "Epoch 52/100, Training Loss: 0.3400, Validation Loss: 0.2410\n",
      "Epoch 53/100, Training Loss: 0.3286, Validation Loss: 0.2397\n",
      "Epoch 54/100, Training Loss: 0.3300, Validation Loss: 0.2394\n",
      "Epoch 55/100, Training Loss: 0.3319, Validation Loss: 0.2381\n",
      "Epoch 56/100, Training Loss: 0.3377, Validation Loss: 0.2387\n",
      "Epoch 57/100, Training Loss: 0.3373, Validation Loss: 0.2390\n",
      "Epoch 58/100, Training Loss: 0.3351, Validation Loss: 0.2394\n",
      "Epoch 59/100, Training Loss: 0.3271, Validation Loss: 0.2393\n",
      "Epoch 60/100, Training Loss: 0.3291, Validation Loss: 0.2396\n",
      "Epoch 61/100, Training Loss: 0.3300, Validation Loss: 0.2396\n",
      "Epoch 62/100, Training Loss: 0.3349, Validation Loss: 0.2389\n",
      "Epoch 63/100, Training Loss: 0.3351, Validation Loss: 0.2383\n",
      "Epoch 64/100, Training Loss: 0.3327, Validation Loss: 0.2382\n",
      "Epoch 65/100, Training Loss: 0.3255, Validation Loss: 0.2377\n",
      "Epoch 66/100, Training Loss: 0.3238, Validation Loss: 0.2374\n",
      "Epoch 67/100, Training Loss: 0.3252, Validation Loss: 0.2375\n",
      "Epoch 68/100, Training Loss: 0.3311, Validation Loss: 0.2373\n",
      "Epoch 69/100, Training Loss: 0.3292, Validation Loss: 0.2366\n",
      "Epoch 70/100, Training Loss: 0.3338, Validation Loss: 0.2364\n",
      "Epoch 71/100, Training Loss: 0.3268, Validation Loss: 0.2367\n",
      "Epoch 72/100, Training Loss: 0.3278, Validation Loss: 0.2365\n",
      "Epoch 73/100, Training Loss: 0.3149, Validation Loss: 0.2380\n",
      "Epoch 74/100, Training Loss: 0.3272, Validation Loss: 0.2386\n",
      "Epoch 75/100, Training Loss: 0.3213, Validation Loss: 0.2385\n",
      "Epoch 76/100, Training Loss: 0.3212, Validation Loss: 0.2390\n",
      "Epoch 77/100, Training Loss: 0.3206, Validation Loss: 0.2380\n",
      "Epoch 78/100, Training Loss: 0.3198, Validation Loss: 0.2377\n",
      "Epoch 79/100, Training Loss: 0.3184, Validation Loss: 0.2364\n",
      "Epoch 80/100, Training Loss: 0.3212, Validation Loss: 0.2360\n",
      "Epoch 81/100, Training Loss: 0.3236, Validation Loss: 0.2361\n",
      "Epoch 82/100, Training Loss: 0.3145, Validation Loss: 0.2358\n",
      "Epoch 83/100, Training Loss: 0.3233, Validation Loss: 0.2358\n",
      "Epoch 84/100, Training Loss: 0.3258, Validation Loss: 0.2359\n",
      "Epoch 85/100, Training Loss: 0.3163, Validation Loss: 0.2368\n",
      "Epoch 86/100, Training Loss: 0.3172, Validation Loss: 0.2363\n",
      "Epoch 87/100, Training Loss: 0.3202, Validation Loss: 0.2365\n",
      "Epoch 88/100, Training Loss: 0.3188, Validation Loss: 0.2364\n",
      "Epoch 89/100, Training Loss: 0.3125, Validation Loss: 0.2353\n",
      "Epoch 90/100, Training Loss: 0.3133, Validation Loss: 0.2340\n",
      "Epoch 91/100, Training Loss: 0.3070, Validation Loss: 0.2337\n",
      "Epoch 92/100, Training Loss: 0.3106, Validation Loss: 0.2331\n",
      "Epoch 93/100, Training Loss: 0.3135, Validation Loss: 0.2330\n",
      "Epoch 94/100, Training Loss: 0.3040, Validation Loss: 0.2338\n",
      "Epoch 95/100, Training Loss: 0.3162, Validation Loss: 0.2341\n",
      "Epoch 96/100, Training Loss: 0.3182, Validation Loss: 0.2348\n",
      "Epoch 97/100, Training Loss: 0.3218, Validation Loss: 0.2354\n",
      "Epoch 98/100, Training Loss: 0.3166, Validation Loss: 0.2365\n",
      "Epoch 99/100, Training Loss: 0.3126, Validation Loss: 0.2372\n",
      "Epoch 100/100, Training Loss: 0.3143, Validation Loss: 0.2366\n",
      "Epoch 1/100, Training Loss: 1.0061, Validation Loss: 0.9743\n",
      "Epoch 2/100, Training Loss: 0.9612, Validation Loss: 0.9218\n",
      "Epoch 3/100, Training Loss: 0.9173, Validation Loss: 0.8674\n",
      "Epoch 4/100, Training Loss: 0.8664, Validation Loss: 0.8075\n",
      "Epoch 5/100, Training Loss: 0.8236, Validation Loss: 0.7393\n",
      "Epoch 6/100, Training Loss: 0.7588, Validation Loss: 0.6644\n",
      "Epoch 7/100, Training Loss: 0.7076, Validation Loss: 0.5862\n",
      "Epoch 8/100, Training Loss: 0.6519, Validation Loss: 0.5094\n",
      "Epoch 9/100, Training Loss: 0.6050, Validation Loss: 0.4415\n",
      "Epoch 10/100, Training Loss: 0.5519, Validation Loss: 0.3861\n",
      "Epoch 11/100, Training Loss: 0.5297, Validation Loss: 0.3433\n",
      "Epoch 12/100, Training Loss: 0.4960, Validation Loss: 0.3138\n",
      "Epoch 13/100, Training Loss: 0.4636, Validation Loss: 0.2937\n",
      "Epoch 14/100, Training Loss: 0.4493, Validation Loss: 0.2811\n",
      "Epoch 15/100, Training Loss: 0.4266, Validation Loss: 0.2744\n",
      "Epoch 16/100, Training Loss: 0.4245, Validation Loss: 0.2712\n",
      "Epoch 17/100, Training Loss: 0.4251, Validation Loss: 0.2700\n",
      "Epoch 18/100, Training Loss: 0.4169, Validation Loss: 0.2674\n",
      "Epoch 19/100, Training Loss: 0.3943, Validation Loss: 0.2634\n",
      "Epoch 20/100, Training Loss: 0.3885, Validation Loss: 0.2581\n",
      "Epoch 21/100, Training Loss: 0.3864, Validation Loss: 0.2542\n",
      "Epoch 22/100, Training Loss: 0.3752, Validation Loss: 0.2520\n",
      "Epoch 23/100, Training Loss: 0.3660, Validation Loss: 0.2499\n",
      "Epoch 24/100, Training Loss: 0.3748, Validation Loss: 0.2493\n",
      "Epoch 25/100, Training Loss: 0.3648, Validation Loss: 0.2487\n",
      "Epoch 26/100, Training Loss: 0.3704, Validation Loss: 0.2474\n",
      "Epoch 27/100, Training Loss: 0.3663, Validation Loss: 0.2462\n",
      "Epoch 28/100, Training Loss: 0.3587, Validation Loss: 0.2447\n",
      "Epoch 29/100, Training Loss: 0.3626, Validation Loss: 0.2451\n",
      "Epoch 30/100, Training Loss: 0.3469, Validation Loss: 0.2455\n",
      "Epoch 31/100, Training Loss: 0.3533, Validation Loss: 0.2448\n",
      "Epoch 32/100, Training Loss: 0.3559, Validation Loss: 0.2429\n",
      "Epoch 33/100, Training Loss: 0.3560, Validation Loss: 0.2423\n",
      "Epoch 34/100, Training Loss: 0.3590, Validation Loss: 0.2428\n",
      "Epoch 35/100, Training Loss: 0.3481, Validation Loss: 0.2419\n",
      "Epoch 36/100, Training Loss: 0.3429, Validation Loss: 0.2394\n",
      "Epoch 37/100, Training Loss: 0.3431, Validation Loss: 0.2376\n",
      "Epoch 38/100, Training Loss: 0.3411, Validation Loss: 0.2384\n",
      "Epoch 39/100, Training Loss: 0.3425, Validation Loss: 0.2387\n",
      "Epoch 40/100, Training Loss: 0.3409, Validation Loss: 0.2404\n",
      "Epoch 41/100, Training Loss: 0.3372, Validation Loss: 0.2417\n",
      "Epoch 42/100, Training Loss: 0.3386, Validation Loss: 0.2413\n",
      "Epoch 43/100, Training Loss: 0.3331, Validation Loss: 0.2376\n",
      "Epoch 44/100, Training Loss: 0.3382, Validation Loss: 0.2344\n",
      "Epoch 45/100, Training Loss: 0.3299, Validation Loss: 0.2338\n",
      "Epoch 46/100, Training Loss: 0.3294, Validation Loss: 0.2347\n",
      "Epoch 47/100, Training Loss: 0.3202, Validation Loss: 0.2360\n",
      "Epoch 48/100, Training Loss: 0.3316, Validation Loss: 0.2362\n",
      "Epoch 49/100, Training Loss: 0.3319, Validation Loss: 0.2366\n",
      "Epoch 50/100, Training Loss: 0.3259, Validation Loss: 0.2357\n",
      "Epoch 51/100, Training Loss: 0.3276, Validation Loss: 0.2350\n",
      "Epoch 52/100, Training Loss: 0.3275, Validation Loss: 0.2362\n",
      "Epoch 53/100, Training Loss: 0.3186, Validation Loss: 0.2344\n",
      "Epoch 54/100, Training Loss: 0.3188, Validation Loss: 0.2334\n",
      "Epoch 55/100, Training Loss: 0.3229, Validation Loss: 0.2328\n",
      "Epoch 56/100, Training Loss: 0.3265, Validation Loss: 0.2355\n",
      "Epoch 57/100, Training Loss: 0.3256, Validation Loss: 0.2367\n",
      "Epoch 58/100, Training Loss: 0.3263, Validation Loss: 0.2361\n",
      "Epoch 59/100, Training Loss: 0.3202, Validation Loss: 0.2349\n",
      "Epoch 60/100, Training Loss: 0.3190, Validation Loss: 0.2343\n",
      "Epoch 61/100, Training Loss: 0.3206, Validation Loss: 0.2343\n",
      "Epoch 62/100, Training Loss: 0.3238, Validation Loss: 0.2335\n",
      "Epoch 63/100, Training Loss: 0.3319, Validation Loss: 0.2334\n",
      "Epoch 64/100, Training Loss: 0.3242, Validation Loss: 0.2344\n",
      "Epoch 65/100, Training Loss: 0.3224, Validation Loss: 0.2348\n",
      "Epoch 66/100, Training Loss: 0.3176, Validation Loss: 0.2336\n",
      "Epoch 67/100, Training Loss: 0.3174, Validation Loss: 0.2334\n",
      "Epoch 68/100, Training Loss: 0.3230, Validation Loss: 0.2331\n",
      "Epoch 69/100, Training Loss: 0.3189, Validation Loss: 0.2318\n",
      "Epoch 70/100, Training Loss: 0.3243, Validation Loss: 0.2324\n",
      "Epoch 71/100, Training Loss: 0.3211, Validation Loss: 0.2341\n",
      "Epoch 72/100, Training Loss: 0.3197, Validation Loss: 0.2336\n",
      "Epoch 73/100, Training Loss: 0.3076, Validation Loss: 0.2340\n",
      "Epoch 74/100, Training Loss: 0.3178, Validation Loss: 0.2342\n",
      "Epoch 75/100, Training Loss: 0.3125, Validation Loss: 0.2331\n",
      "Epoch 76/100, Training Loss: 0.3142, Validation Loss: 0.2329\n",
      "Epoch 77/100, Training Loss: 0.3140, Validation Loss: 0.2328\n",
      "Epoch 78/100, Training Loss: 0.3096, Validation Loss: 0.2329\n",
      "Epoch 79/100, Training Loss: 0.3131, Validation Loss: 0.2324\n",
      "Epoch 80/100, Training Loss: 0.3137, Validation Loss: 0.2324\n",
      "Epoch 81/100, Training Loss: 0.3143, Validation Loss: 0.2330\n",
      "Epoch 82/100, Training Loss: 0.3099, Validation Loss: 0.2327\n",
      "Epoch 83/100, Training Loss: 0.3189, Validation Loss: 0.2316\n",
      "Epoch 84/100, Training Loss: 0.3184, Validation Loss: 0.2309\n",
      "Epoch 85/100, Training Loss: 0.3068, Validation Loss: 0.2319\n",
      "Epoch 86/100, Training Loss: 0.3109, Validation Loss: 0.2316\n",
      "Epoch 87/100, Training Loss: 0.3137, Validation Loss: 0.2321\n",
      "Epoch 88/100, Training Loss: 0.3093, Validation Loss: 0.2317\n",
      "Epoch 89/100, Training Loss: 0.3090, Validation Loss: 0.2298\n",
      "Epoch 90/100, Training Loss: 0.3079, Validation Loss: 0.2294\n",
      "Epoch 91/100, Training Loss: 0.2981, Validation Loss: 0.2296\n",
      "Epoch 92/100, Training Loss: 0.3034, Validation Loss: 0.2298\n",
      "Epoch 93/100, Training Loss: 0.3063, Validation Loss: 0.2307\n",
      "Epoch 94/100, Training Loss: 0.2966, Validation Loss: 0.2316\n",
      "Epoch 95/100, Training Loss: 0.3059, Validation Loss: 0.2312\n",
      "Epoch 96/100, Training Loss: 0.3105, Validation Loss: 0.2310\n",
      "Epoch 97/100, Training Loss: 0.3161, Validation Loss: 0.2321\n",
      "Epoch 98/100, Training Loss: 0.3093, Validation Loss: 0.2343\n",
      "Epoch 99/100, Training Loss: 0.3067, Validation Loss: 0.2341\n",
      "Epoch 100/100, Training Loss: 0.3042, Validation Loss: 0.2317\n",
      "Epoch 1/100, Training Loss: 1.3311, Validation Loss: 1.1221\n",
      "Epoch 2/100, Training Loss: 1.1367, Validation Loss: 1.0547\n",
      "Epoch 3/100, Training Loss: 0.9717, Validation Loss: 0.9743\n",
      "Epoch 4/100, Training Loss: 0.8600, Validation Loss: 0.8866\n",
      "Epoch 5/100, Training Loss: 0.7602, Validation Loss: 0.8001\n",
      "Epoch 6/100, Training Loss: 0.6753, Validation Loss: 0.7240\n",
      "Epoch 7/100, Training Loss: 0.6320, Validation Loss: 0.6619\n",
      "Epoch 8/100, Training Loss: 0.5976, Validation Loss: 0.6137\n",
      "Epoch 9/100, Training Loss: 0.5445, Validation Loss: 0.5775\n",
      "Epoch 10/100, Training Loss: 0.5399, Validation Loss: 0.5513\n",
      "Epoch 11/100, Training Loss: 0.4877, Validation Loss: 0.5309\n",
      "Epoch 12/100, Training Loss: 0.4741, Validation Loss: 0.5143\n",
      "Epoch 13/100, Training Loss: 0.4499, Validation Loss: 0.5010\n",
      "Epoch 14/100, Training Loss: 0.4495, Validation Loss: 0.4910\n",
      "Epoch 15/100, Training Loss: 0.4312, Validation Loss: 0.4814\n",
      "Epoch 16/100, Training Loss: 0.4322, Validation Loss: 0.4740\n",
      "Epoch 17/100, Training Loss: 0.4125, Validation Loss: 0.4676\n",
      "Epoch 18/100, Training Loss: 0.4048, Validation Loss: 0.4623\n",
      "Epoch 19/100, Training Loss: 0.3966, Validation Loss: 0.4586\n",
      "Epoch 20/100, Training Loss: 0.3935, Validation Loss: 0.4554\n",
      "Epoch 21/100, Training Loss: 0.3828, Validation Loss: 0.4530\n",
      "Epoch 22/100, Training Loss: 0.3722, Validation Loss: 0.4503\n",
      "Epoch 23/100, Training Loss: 0.3566, Validation Loss: 0.4475\n",
      "Epoch 24/100, Training Loss: 0.3579, Validation Loss: 0.4449\n",
      "Epoch 25/100, Training Loss: 0.3490, Validation Loss: 0.4414\n",
      "Epoch 26/100, Training Loss: 0.3485, Validation Loss: 0.4391\n",
      "Epoch 27/100, Training Loss: 0.3388, Validation Loss: 0.4372\n",
      "Epoch 28/100, Training Loss: 0.3435, Validation Loss: 0.4361\n",
      "Epoch 29/100, Training Loss: 0.3325, Validation Loss: 0.4359\n",
      "Epoch 30/100, Training Loss: 0.3313, Validation Loss: 0.4352\n",
      "Epoch 31/100, Training Loss: 0.3257, Validation Loss: 0.4356\n",
      "Epoch 32/100, Training Loss: 0.3209, Validation Loss: 0.4349\n",
      "Epoch 33/100, Training Loss: 0.3353, Validation Loss: 0.4332\n",
      "Epoch 34/100, Training Loss: 0.3222, Validation Loss: 0.4320\n",
      "Epoch 35/100, Training Loss: 0.3157, Validation Loss: 0.4305\n",
      "Epoch 36/100, Training Loss: 0.3142, Validation Loss: 0.4288\n",
      "Epoch 37/100, Training Loss: 0.3008, Validation Loss: 0.4280\n",
      "Epoch 38/100, Training Loss: 0.3025, Validation Loss: 0.4270\n",
      "Epoch 39/100, Training Loss: 0.3060, Validation Loss: 0.4268\n",
      "Epoch 40/100, Training Loss: 0.3032, Validation Loss: 0.4265\n",
      "Epoch 41/100, Training Loss: 0.2944, Validation Loss: 0.4257\n",
      "Epoch 42/100, Training Loss: 0.2977, Validation Loss: 0.4247\n",
      "Epoch 43/100, Training Loss: 0.3048, Validation Loss: 0.4239\n",
      "Epoch 44/100, Training Loss: 0.2997, Validation Loss: 0.4238\n",
      "Epoch 45/100, Training Loss: 0.2970, Validation Loss: 0.4241\n",
      "Epoch 46/100, Training Loss: 0.2959, Validation Loss: 0.4237\n",
      "Epoch 47/100, Training Loss: 0.2906, Validation Loss: 0.4237\n",
      "Epoch 48/100, Training Loss: 0.2890, Validation Loss: 0.4239\n",
      "Epoch 49/100, Training Loss: 0.2910, Validation Loss: 0.4234\n",
      "Epoch 50/100, Training Loss: 0.2999, Validation Loss: 0.4236\n",
      "Epoch 51/100, Training Loss: 0.2901, Validation Loss: 0.4240\n",
      "Epoch 52/100, Training Loss: 0.2903, Validation Loss: 0.4240\n",
      "Epoch 53/100, Training Loss: 0.2826, Validation Loss: 0.4242\n",
      "Epoch 54/100, Training Loss: 0.2892, Validation Loss: 0.4238\n",
      "Epoch 55/100, Training Loss: 0.2851, Validation Loss: 0.4231\n",
      "Epoch 56/100, Training Loss: 0.2841, Validation Loss: 0.4233\n",
      "Epoch 57/100, Training Loss: 0.2866, Validation Loss: 0.4234\n",
      "Epoch 58/100, Training Loss: 0.2856, Validation Loss: 0.4228\n",
      "Epoch 59/100, Training Loss: 0.2813, Validation Loss: 0.4230\n",
      "Epoch 60/100, Training Loss: 0.2875, Validation Loss: 0.4230\n",
      "Epoch 61/100, Training Loss: 0.2694, Validation Loss: 0.4231\n",
      "Epoch 62/100, Training Loss: 0.2903, Validation Loss: 0.4222\n",
      "Epoch 63/100, Training Loss: 0.2775, Validation Loss: 0.4223\n",
      "Epoch 64/100, Training Loss: 0.2790, Validation Loss: 0.4222\n",
      "Epoch 65/100, Training Loss: 0.2715, Validation Loss: 0.4218\n",
      "Epoch 66/100, Training Loss: 0.2762, Validation Loss: 0.4215\n",
      "Epoch 67/100, Training Loss: 0.2838, Validation Loss: 0.4212\n",
      "Epoch 68/100, Training Loss: 0.2700, Validation Loss: 0.4208\n",
      "Epoch 69/100, Training Loss: 0.2778, Validation Loss: 0.4207\n",
      "Epoch 70/100, Training Loss: 0.2738, Validation Loss: 0.4208\n",
      "Epoch 71/100, Training Loss: 0.2690, Validation Loss: 0.4204\n",
      "Epoch 72/100, Training Loss: 0.2721, Validation Loss: 0.4205\n",
      "Epoch 73/100, Training Loss: 0.2753, Validation Loss: 0.4207\n",
      "Epoch 74/100, Training Loss: 0.2660, Validation Loss: 0.4199\n",
      "Epoch 75/100, Training Loss: 0.2741, Validation Loss: 0.4193\n",
      "Epoch 76/100, Training Loss: 0.2763, Validation Loss: 0.4195\n",
      "Epoch 77/100, Training Loss: 0.2746, Validation Loss: 0.4194\n",
      "Epoch 78/100, Training Loss: 0.2676, Validation Loss: 0.4190\n",
      "Epoch 79/100, Training Loss: 0.2772, Validation Loss: 0.4192\n",
      "Epoch 80/100, Training Loss: 0.2623, Validation Loss: 0.4195\n",
      "Epoch 81/100, Training Loss: 0.2696, Validation Loss: 0.4191\n",
      "Epoch 82/100, Training Loss: 0.2636, Validation Loss: 0.4192\n",
      "Epoch 83/100, Training Loss: 0.2708, Validation Loss: 0.4193\n",
      "Epoch 84/100, Training Loss: 0.2690, Validation Loss: 0.4198\n",
      "Epoch 85/100, Training Loss: 0.2651, Validation Loss: 0.4196\n",
      "Epoch 86/100, Training Loss: 0.2734, Validation Loss: 0.4198\n",
      "Epoch 87/100, Training Loss: 0.2661, Validation Loss: 0.4196\n",
      "Epoch 88/100, Training Loss: 0.2625, Validation Loss: 0.4192\n",
      "Epoch 89/100, Training Loss: 0.2696, Validation Loss: 0.4197\n",
      "Epoch 90/100, Training Loss: 0.2670, Validation Loss: 0.4200\n",
      "Epoch 91/100, Training Loss: 0.2684, Validation Loss: 0.4199\n",
      "Epoch 92/100, Training Loss: 0.2640, Validation Loss: 0.4204\n",
      "Epoch 93/100, Training Loss: 0.2598, Validation Loss: 0.4202\n",
      "Epoch 94/100, Training Loss: 0.2653, Validation Loss: 0.4196\n",
      "Epoch 95/100, Training Loss: 0.2649, Validation Loss: 0.4193\n",
      "Epoch 96/100, Training Loss: 0.2698, Validation Loss: 0.4193\n",
      "Epoch 97/100, Training Loss: 0.2629, Validation Loss: 0.4195\n",
      "Epoch 98/100, Training Loss: 0.2622, Validation Loss: 0.4194\n",
      "Epoch 99/100, Training Loss: 0.2710, Validation Loss: 0.4194\n",
      "Epoch 100/100, Training Loss: 0.2604, Validation Loss: 0.4188\n",
      "Epoch 1/100, Training Loss: 1.0133, Validation Loss: 1.1238\n",
      "Epoch 2/100, Training Loss: 0.9649, Validation Loss: 1.0812\n",
      "Epoch 3/100, Training Loss: 0.9167, Validation Loss: 1.0388\n",
      "Epoch 4/100, Training Loss: 0.8749, Validation Loss: 0.9928\n",
      "Epoch 5/100, Training Loss: 0.8200, Validation Loss: 0.9401\n",
      "Epoch 6/100, Training Loss: 0.7657, Validation Loss: 0.8808\n",
      "Epoch 7/100, Training Loss: 0.7093, Validation Loss: 0.8157\n",
      "Epoch 8/100, Training Loss: 0.6535, Validation Loss: 0.7488\n",
      "Epoch 9/100, Training Loss: 0.5932, Validation Loss: 0.6848\n",
      "Epoch 10/100, Training Loss: 0.5346, Validation Loss: 0.6282\n",
      "Epoch 11/100, Training Loss: 0.4860, Validation Loss: 0.5826\n",
      "Epoch 12/100, Training Loss: 0.4589, Validation Loss: 0.5478\n",
      "Epoch 13/100, Training Loss: 0.4284, Validation Loss: 0.5224\n",
      "Epoch 14/100, Training Loss: 0.4223, Validation Loss: 0.5032\n",
      "Epoch 15/100, Training Loss: 0.4003, Validation Loss: 0.4875\n",
      "Epoch 16/100, Training Loss: 0.3903, Validation Loss: 0.4764\n",
      "Epoch 17/100, Training Loss: 0.3716, Validation Loss: 0.4686\n",
      "Epoch 18/100, Training Loss: 0.3576, Validation Loss: 0.4614\n",
      "Epoch 19/100, Training Loss: 0.3504, Validation Loss: 0.4552\n",
      "Epoch 20/100, Training Loss: 0.3496, Validation Loss: 0.4498\n",
      "Epoch 21/100, Training Loss: 0.3444, Validation Loss: 0.4462\n",
      "Epoch 22/100, Training Loss: 0.3282, Validation Loss: 0.4423\n",
      "Epoch 23/100, Training Loss: 0.3192, Validation Loss: 0.4383\n",
      "Epoch 24/100, Training Loss: 0.3185, Validation Loss: 0.4349\n",
      "Epoch 25/100, Training Loss: 0.3117, Validation Loss: 0.4320\n",
      "Epoch 26/100, Training Loss: 0.3104, Validation Loss: 0.4304\n",
      "Epoch 27/100, Training Loss: 0.3093, Validation Loss: 0.4300\n",
      "Epoch 28/100, Training Loss: 0.3088, Validation Loss: 0.4315\n",
      "Epoch 29/100, Training Loss: 0.3004, Validation Loss: 0.4334\n",
      "Epoch 30/100, Training Loss: 0.3003, Validation Loss: 0.4334\n",
      "Epoch 31/100, Training Loss: 0.2922, Validation Loss: 0.4313\n",
      "Epoch 32/100, Training Loss: 0.2940, Validation Loss: 0.4291\n",
      "Epoch 33/100, Training Loss: 0.3090, Validation Loss: 0.4267\n",
      "Epoch 34/100, Training Loss: 0.2929, Validation Loss: 0.4255\n",
      "Epoch 35/100, Training Loss: 0.2897, Validation Loss: 0.4245\n",
      "Epoch 36/100, Training Loss: 0.2863, Validation Loss: 0.4237\n",
      "Epoch 37/100, Training Loss: 0.2798, Validation Loss: 0.4240\n",
      "Epoch 38/100, Training Loss: 0.2804, Validation Loss: 0.4236\n",
      "Epoch 39/100, Training Loss: 0.2839, Validation Loss: 0.4237\n",
      "Epoch 40/100, Training Loss: 0.2784, Validation Loss: 0.4232\n",
      "Epoch 41/100, Training Loss: 0.2753, Validation Loss: 0.4218\n",
      "Epoch 42/100, Training Loss: 0.2797, Validation Loss: 0.4210\n",
      "Epoch 43/100, Training Loss: 0.2870, Validation Loss: 0.4211\n",
      "Epoch 44/100, Training Loss: 0.2809, Validation Loss: 0.4220\n",
      "Epoch 45/100, Training Loss: 0.2761, Validation Loss: 0.4230\n",
      "Epoch 46/100, Training Loss: 0.2780, Validation Loss: 0.4220\n",
      "Epoch 47/100, Training Loss: 0.2727, Validation Loss: 0.4214\n",
      "Epoch 48/100, Training Loss: 0.2742, Validation Loss: 0.4205\n",
      "Epoch 49/100, Training Loss: 0.2722, Validation Loss: 0.4200\n",
      "Epoch 50/100, Training Loss: 0.2810, Validation Loss: 0.4208\n",
      "Epoch 51/100, Training Loss: 0.2767, Validation Loss: 0.4216\n",
      "Epoch 52/100, Training Loss: 0.2739, Validation Loss: 0.4223\n",
      "Epoch 53/100, Training Loss: 0.2671, Validation Loss: 0.4226\n",
      "Epoch 54/100, Training Loss: 0.2750, Validation Loss: 0.4213\n",
      "Epoch 55/100, Training Loss: 0.2733, Validation Loss: 0.4204\n",
      "Epoch 56/100, Training Loss: 0.2664, Validation Loss: 0.4209\n",
      "Epoch 57/100, Training Loss: 0.2749, Validation Loss: 0.4214\n",
      "Epoch 58/100, Training Loss: 0.2694, Validation Loss: 0.4214\n",
      "Epoch 59/100, Training Loss: 0.2664, Validation Loss: 0.4205\n",
      "Epoch 60/100, Training Loss: 0.2740, Validation Loss: 0.4196\n",
      "Epoch 61/100, Training Loss: 0.2587, Validation Loss: 0.4197\n",
      "Epoch 62/100, Training Loss: 0.2794, Validation Loss: 0.4194\n",
      "Epoch 63/100, Training Loss: 0.2636, Validation Loss: 0.4190\n",
      "Epoch 64/100, Training Loss: 0.2688, Validation Loss: 0.4197\n",
      "Epoch 65/100, Training Loss: 0.2566, Validation Loss: 0.4196\n",
      "Epoch 66/100, Training Loss: 0.2641, Validation Loss: 0.4191\n",
      "Epoch 67/100, Training Loss: 0.2686, Validation Loss: 0.4189\n",
      "Epoch 68/100, Training Loss: 0.2634, Validation Loss: 0.4184\n",
      "Epoch 69/100, Training Loss: 0.2674, Validation Loss: 0.4193\n",
      "Epoch 70/100, Training Loss: 0.2649, Validation Loss: 0.4200\n",
      "Epoch 71/100, Training Loss: 0.2582, Validation Loss: 0.4199\n",
      "Epoch 72/100, Training Loss: 0.2594, Validation Loss: 0.4192\n",
      "Epoch 73/100, Training Loss: 0.2661, Validation Loss: 0.4186\n",
      "Epoch 74/100, Training Loss: 0.2577, Validation Loss: 0.4176\n",
      "Epoch 75/100, Training Loss: 0.2677, Validation Loss: 0.4174\n",
      "Epoch 76/100, Training Loss: 0.2662, Validation Loss: 0.4183\n",
      "Epoch 77/100, Training Loss: 0.2666, Validation Loss: 0.4186\n",
      "Epoch 78/100, Training Loss: 0.2558, Validation Loss: 0.4183\n",
      "Epoch 79/100, Training Loss: 0.2667, Validation Loss: 0.4176\n",
      "Epoch 80/100, Training Loss: 0.2555, Validation Loss: 0.4179\n",
      "Epoch 81/100, Training Loss: 0.2601, Validation Loss: 0.4179\n",
      "Epoch 82/100, Training Loss: 0.2573, Validation Loss: 0.4176\n",
      "Epoch 83/100, Training Loss: 0.2614, Validation Loss: 0.4182\n",
      "Epoch 84/100, Training Loss: 0.2587, Validation Loss: 0.4198\n",
      "Epoch 85/100, Training Loss: 0.2510, Validation Loss: 0.4199\n",
      "Epoch 86/100, Training Loss: 0.2641, Validation Loss: 0.4182\n",
      "Epoch 87/100, Training Loss: 0.2549, Validation Loss: 0.4171\n",
      "Epoch 88/100, Training Loss: 0.2507, Validation Loss: 0.4172\n",
      "Epoch 89/100, Training Loss: 0.2608, Validation Loss: 0.4184\n",
      "Epoch 90/100, Training Loss: 0.2583, Validation Loss: 0.4204\n",
      "Epoch 91/100, Training Loss: 0.2612, Validation Loss: 0.4213\n",
      "Epoch 92/100, Training Loss: 0.2561, Validation Loss: 0.4206\n",
      "Epoch 93/100, Training Loss: 0.2539, Validation Loss: 0.4183\n",
      "Epoch 94/100, Training Loss: 0.2563, Validation Loss: 0.4173\n",
      "Epoch 95/100, Training Loss: 0.2587, Validation Loss: 0.4177\n",
      "Epoch 96/100, Training Loss: 0.2624, Validation Loss: 0.4188\n",
      "Epoch 97/100, Training Loss: 0.2522, Validation Loss: 0.4193\n",
      "Epoch 98/100, Training Loss: 0.2570, Validation Loss: 0.4186\n",
      "Epoch 99/100, Training Loss: 0.2637, Validation Loss: 0.4180\n",
      "Epoch 100/100, Training Loss: 0.2545, Validation Loss: 0.4173\n",
      "Epoch 1/100, Training Loss: 0.7315, Validation Loss: 0.4840\n",
      "Epoch 2/100, Training Loss: 0.4642, Validation Loss: 0.4513\n",
      "Epoch 3/100, Training Loss: 0.3935, Validation Loss: 0.4377\n",
      "Epoch 4/100, Training Loss: 0.3684, Validation Loss: 0.4447\n",
      "Epoch 5/100, Training Loss: 0.3523, Validation Loss: 0.4318\n",
      "Epoch 6/100, Training Loss: 0.3456, Validation Loss: 0.4343\n",
      "Epoch 7/100, Training Loss: 0.3464, Validation Loss: 0.4316\n",
      "Epoch 8/100, Training Loss: 0.3436, Validation Loss: 0.4389\n",
      "Epoch 9/100, Training Loss: 0.3288, Validation Loss: 0.4337\n",
      "Epoch 10/100, Training Loss: 0.3233, Validation Loss: 0.4212\n",
      "Epoch 11/100, Training Loss: 0.3319, Validation Loss: 0.4224\n",
      "Epoch 12/100, Training Loss: 0.3337, Validation Loss: 0.4193\n",
      "Epoch 13/100, Training Loss: 0.3288, Validation Loss: 0.4371\n",
      "Epoch 14/100, Training Loss: 0.3265, Validation Loss: 0.4285\n",
      "Epoch 15/100, Training Loss: 0.3204, Validation Loss: 0.4292\n",
      "Epoch 16/100, Training Loss: 0.3137, Validation Loss: 0.4222\n",
      "Epoch 17/100, Training Loss: 0.3207, Validation Loss: 0.4244\n",
      "Epoch 18/100, Training Loss: 0.3219, Validation Loss: 0.4283\n",
      "Epoch 19/100, Training Loss: 0.3162, Validation Loss: 0.4227\n",
      "Epoch 20/100, Training Loss: 0.3181, Validation Loss: 0.4244\n",
      "Epoch 21/100, Training Loss: 0.3210, Validation Loss: 0.4259\n",
      "Epoch 22/100, Training Loss: 0.3180, Validation Loss: 0.4267\n",
      "Epoch 23/100, Training Loss: 0.3103, Validation Loss: 0.4206\n",
      "Epoch 24/100, Training Loss: 0.3114, Validation Loss: 0.4216\n",
      "Epoch 25/100, Training Loss: 0.3151, Validation Loss: 0.4279\n",
      "Epoch 26/100, Training Loss: 0.3006, Validation Loss: 0.4243\n",
      "Epoch 27/100, Training Loss: 0.3082, Validation Loss: 0.4283\n",
      "Epoch 28/100, Training Loss: 0.3062, Validation Loss: 0.4255\n",
      "Epoch 29/100, Training Loss: 0.3210, Validation Loss: 0.4241\n",
      "Epoch 30/100, Training Loss: 0.3112, Validation Loss: 0.4233\n",
      "Epoch 31/100, Training Loss: 0.3196, Validation Loss: 0.4310\n",
      "Epoch 32/100, Training Loss: 0.3082, Validation Loss: 0.4243\n",
      "Epoch 33/100, Training Loss: 0.3013, Validation Loss: 0.4220\n",
      "Epoch 34/100, Training Loss: 0.3096, Validation Loss: 0.4289\n",
      "Epoch 35/100, Training Loss: 0.3093, Validation Loss: 0.4215\n",
      "Epoch 36/100, Training Loss: 0.3031, Validation Loss: 0.4229\n",
      "Epoch 37/100, Training Loss: 0.3116, Validation Loss: 0.4252\n",
      "Epoch 38/100, Training Loss: 0.3103, Validation Loss: 0.4210\n",
      "Epoch 39/100, Training Loss: 0.3201, Validation Loss: 0.4219\n",
      "Epoch 40/100, Training Loss: 0.3063, Validation Loss: 0.4332\n",
      "Epoch 41/100, Training Loss: 0.3059, Validation Loss: 0.4242\n",
      "Epoch 42/100, Training Loss: 0.3139, Validation Loss: 0.4200\n",
      "Epoch 43/100, Training Loss: 0.3049, Validation Loss: 0.4200\n",
      "Epoch 44/100, Training Loss: 0.3041, Validation Loss: 0.4240\n",
      "Epoch 45/100, Training Loss: 0.3070, Validation Loss: 0.4232\n",
      "Epoch 46/100, Training Loss: 0.3025, Validation Loss: 0.4205\n",
      "Epoch 47/100, Training Loss: 0.3035, Validation Loss: 0.4192\n",
      "Epoch 48/100, Training Loss: 0.2957, Validation Loss: 0.4244\n",
      "Epoch 49/100, Training Loss: 0.3067, Validation Loss: 0.4244\n",
      "Epoch 50/100, Training Loss: 0.3064, Validation Loss: 0.4310\n",
      "Epoch 51/100, Training Loss: 0.3106, Validation Loss: 0.4267\n",
      "Epoch 52/100, Training Loss: 0.3013, Validation Loss: 0.4224\n",
      "Epoch 53/100, Training Loss: 0.3028, Validation Loss: 0.4265\n",
      "Epoch 54/100, Training Loss: 0.2968, Validation Loss: 0.4233\n",
      "Epoch 55/100, Training Loss: 0.3071, Validation Loss: 0.4229\n",
      "Epoch 56/100, Training Loss: 0.3012, Validation Loss: 0.4251\n",
      "Epoch 57/100, Training Loss: 0.3047, Validation Loss: 0.4203\n",
      "Epoch 58/100, Training Loss: 0.3103, Validation Loss: 0.4236\n",
      "Epoch 59/100, Training Loss: 0.3085, Validation Loss: 0.4250\n",
      "Epoch 60/100, Training Loss: 0.3025, Validation Loss: 0.4261\n",
      "Epoch 61/100, Training Loss: 0.3020, Validation Loss: 0.4331\n",
      "Epoch 62/100, Training Loss: 0.3103, Validation Loss: 0.4229\n",
      "Epoch 63/100, Training Loss: 0.3094, Validation Loss: 0.4246\n",
      "Epoch 64/100, Training Loss: 0.3009, Validation Loss: 0.4265\n",
      "Epoch 65/100, Training Loss: 0.2916, Validation Loss: 0.4234\n",
      "Epoch 66/100, Training Loss: 0.3079, Validation Loss: 0.4217\n",
      "Epoch 67/100, Training Loss: 0.3032, Validation Loss: 0.4348\n",
      "Epoch 68/100, Training Loss: 0.3092, Validation Loss: 0.4302\n",
      "Epoch 69/100, Training Loss: 0.2987, Validation Loss: 0.4263\n",
      "Epoch 70/100, Training Loss: 0.2976, Validation Loss: 0.4230\n",
      "Epoch 71/100, Training Loss: 0.3072, Validation Loss: 0.4229\n",
      "Epoch 72/100, Training Loss: 0.2995, Validation Loss: 0.4254\n",
      "Epoch 73/100, Training Loss: 0.2976, Validation Loss: 0.4273\n",
      "Epoch 74/100, Training Loss: 0.3081, Validation Loss: 0.4295\n",
      "Epoch 75/100, Training Loss: 0.3127, Validation Loss: 0.4270\n",
      "Epoch 76/100, Training Loss: 0.3067, Validation Loss: 0.4230\n",
      "Epoch 77/100, Training Loss: 0.2944, Validation Loss: 0.4230\n",
      "Epoch 78/100, Training Loss: 0.2944, Validation Loss: 0.4286\n",
      "Epoch 79/100, Training Loss: 0.3070, Validation Loss: 0.4224\n",
      "Epoch 80/100, Training Loss: 0.2947, Validation Loss: 0.4342\n",
      "Epoch 81/100, Training Loss: 0.2958, Validation Loss: 0.4291\n",
      "Epoch 82/100, Training Loss: 0.2970, Validation Loss: 0.4266\n",
      "Epoch 83/100, Training Loss: 0.2967, Validation Loss: 0.4245\n",
      "Epoch 84/100, Training Loss: 0.3083, Validation Loss: 0.4243\n",
      "Epoch 85/100, Training Loss: 0.2926, Validation Loss: 0.4247\n",
      "Epoch 86/100, Training Loss: 0.2926, Validation Loss: 0.4272\n",
      "Epoch 87/100, Training Loss: 0.3081, Validation Loss: 0.4226\n",
      "Epoch 88/100, Training Loss: 0.3019, Validation Loss: 0.4276\n",
      "Epoch 89/100, Training Loss: 0.3089, Validation Loss: 0.4257\n",
      "Epoch 90/100, Training Loss: 0.3046, Validation Loss: 0.4252\n",
      "Epoch 91/100, Training Loss: 0.3167, Validation Loss: 0.4226\n",
      "Epoch 92/100, Training Loss: 0.2955, Validation Loss: 0.4210\n",
      "Epoch 93/100, Training Loss: 0.2944, Validation Loss: 0.4244\n",
      "Epoch 94/100, Training Loss: 0.3046, Validation Loss: 0.4211\n",
      "Epoch 95/100, Training Loss: 0.2957, Validation Loss: 0.4215\n",
      "Epoch 96/100, Training Loss: 0.2936, Validation Loss: 0.4236\n",
      "Epoch 97/100, Training Loss: 0.3029, Validation Loss: 0.4268\n",
      "Epoch 98/100, Training Loss: 0.2942, Validation Loss: 0.4205\n",
      "Epoch 99/100, Training Loss: 0.2999, Validation Loss: 0.4272\n",
      "Epoch 100/100, Training Loss: 0.2970, Validation Loss: 0.4198\n",
      "Epoch 1/100, Training Loss: 0.5943, Validation Loss: 0.4394\n",
      "Epoch 2/100, Training Loss: 0.3409, Validation Loss: 0.4244\n",
      "Epoch 3/100, Training Loss: 0.3057, Validation Loss: 0.4230\n",
      "Epoch 4/100, Training Loss: 0.2992, Validation Loss: 0.4280\n",
      "Epoch 5/100, Training Loss: 0.2911, Validation Loss: 0.4243\n",
      "Epoch 6/100, Training Loss: 0.2895, Validation Loss: 0.4232\n",
      "Epoch 7/100, Training Loss: 0.2958, Validation Loss: 0.4249\n",
      "Epoch 8/100, Training Loss: 0.2909, Validation Loss: 0.4257\n",
      "Epoch 9/100, Training Loss: 0.2808, Validation Loss: 0.4229\n",
      "Epoch 10/100, Training Loss: 0.2832, Validation Loss: 0.4235\n",
      "Epoch 11/100, Training Loss: 0.2841, Validation Loss: 0.4236\n",
      "Epoch 12/100, Training Loss: 0.2841, Validation Loss: 0.4227\n",
      "Epoch 13/100, Training Loss: 0.2758, Validation Loss: 0.4324\n",
      "Epoch 14/100, Training Loss: 0.2825, Validation Loss: 0.4257\n",
      "Epoch 15/100, Training Loss: 0.2800, Validation Loss: 0.4291\n",
      "Epoch 16/100, Training Loss: 0.2731, Validation Loss: 0.4247\n",
      "Epoch 17/100, Training Loss: 0.2748, Validation Loss: 0.4235\n",
      "Epoch 18/100, Training Loss: 0.2724, Validation Loss: 0.4284\n",
      "Epoch 19/100, Training Loss: 0.2794, Validation Loss: 0.4227\n",
      "Epoch 20/100, Training Loss: 0.2727, Validation Loss: 0.4261\n",
      "Epoch 21/100, Training Loss: 0.2775, Validation Loss: 0.4235\n",
      "Epoch 22/100, Training Loss: 0.2744, Validation Loss: 0.4230\n",
      "Epoch 23/100, Training Loss: 0.2740, Validation Loss: 0.4206\n",
      "Epoch 24/100, Training Loss: 0.2720, Validation Loss: 0.4236\n",
      "Epoch 25/100, Training Loss: 0.2749, Validation Loss: 0.4234\n",
      "Epoch 26/100, Training Loss: 0.2660, Validation Loss: 0.4256\n",
      "Epoch 27/100, Training Loss: 0.2708, Validation Loss: 0.4247\n",
      "Epoch 28/100, Training Loss: 0.2644, Validation Loss: 0.4265\n",
      "Epoch 29/100, Training Loss: 0.2717, Validation Loss: 0.4225\n",
      "Epoch 30/100, Training Loss: 0.2680, Validation Loss: 0.4258\n",
      "Epoch 31/100, Training Loss: 0.2718, Validation Loss: 0.4287\n",
      "Epoch 32/100, Training Loss: 0.2777, Validation Loss: 0.4241\n",
      "Epoch 33/100, Training Loss: 0.2631, Validation Loss: 0.4245\n",
      "Epoch 34/100, Training Loss: 0.2685, Validation Loss: 0.4232\n",
      "Epoch 35/100, Training Loss: 0.2678, Validation Loss: 0.4255\n",
      "Epoch 36/100, Training Loss: 0.2677, Validation Loss: 0.4237\n",
      "Epoch 37/100, Training Loss: 0.2729, Validation Loss: 0.4227\n",
      "Epoch 38/100, Training Loss: 0.2732, Validation Loss: 0.4223\n",
      "Epoch 39/100, Training Loss: 0.2635, Validation Loss: 0.4232\n",
      "Epoch 40/100, Training Loss: 0.2636, Validation Loss: 0.4244\n",
      "Epoch 41/100, Training Loss: 0.2650, Validation Loss: 0.4286\n",
      "Epoch 42/100, Training Loss: 0.2705, Validation Loss: 0.4237\n",
      "Epoch 43/100, Training Loss: 0.2672, Validation Loss: 0.4216\n",
      "Epoch 44/100, Training Loss: 0.2691, Validation Loss: 0.4251\n",
      "Epoch 45/100, Training Loss: 0.2655, Validation Loss: 0.4242\n",
      "Epoch 46/100, Training Loss: 0.2686, Validation Loss: 0.4240\n",
      "Epoch 47/100, Training Loss: 0.2669, Validation Loss: 0.4210\n",
      "Epoch 48/100, Training Loss: 0.2594, Validation Loss: 0.4252\n",
      "Epoch 49/100, Training Loss: 0.2669, Validation Loss: 0.4247\n",
      "Epoch 50/100, Training Loss: 0.2644, Validation Loss: 0.4295\n",
      "Epoch 51/100, Training Loss: 0.2686, Validation Loss: 0.4264\n",
      "Epoch 52/100, Training Loss: 0.2656, Validation Loss: 0.4244\n",
      "Epoch 53/100, Training Loss: 0.2610, Validation Loss: 0.4226\n",
      "Epoch 54/100, Training Loss: 0.2616, Validation Loss: 0.4259\n",
      "Epoch 55/100, Training Loss: 0.2678, Validation Loss: 0.4252\n",
      "Epoch 56/100, Training Loss: 0.2663, Validation Loss: 0.4243\n",
      "Epoch 57/100, Training Loss: 0.2596, Validation Loss: 0.4238\n",
      "Epoch 58/100, Training Loss: 0.2729, Validation Loss: 0.4250\n",
      "Epoch 59/100, Training Loss: 0.2671, Validation Loss: 0.4239\n",
      "Epoch 60/100, Training Loss: 0.2674, Validation Loss: 0.4218\n",
      "Epoch 61/100, Training Loss: 0.2648, Validation Loss: 0.4282\n",
      "Epoch 62/100, Training Loss: 0.2638, Validation Loss: 0.4243\n",
      "Epoch 63/100, Training Loss: 0.2695, Validation Loss: 0.4258\n",
      "Epoch 64/100, Training Loss: 0.2675, Validation Loss: 0.4206\n",
      "Epoch 65/100, Training Loss: 0.2546, Validation Loss: 0.4250\n",
      "Epoch 66/100, Training Loss: 0.2613, Validation Loss: 0.4242\n",
      "Epoch 67/100, Training Loss: 0.2659, Validation Loss: 0.4333\n",
      "Epoch 68/100, Training Loss: 0.2690, Validation Loss: 0.4276\n",
      "Epoch 69/100, Training Loss: 0.2620, Validation Loss: 0.4265\n",
      "Epoch 70/100, Training Loss: 0.2547, Validation Loss: 0.4244\n",
      "Epoch 71/100, Training Loss: 0.2708, Validation Loss: 0.4264\n",
      "Epoch 72/100, Training Loss: 0.2655, Validation Loss: 0.4262\n",
      "Epoch 73/100, Training Loss: 0.2611, Validation Loss: 0.4311\n",
      "Epoch 74/100, Training Loss: 0.2662, Validation Loss: 0.4260\n",
      "Epoch 75/100, Training Loss: 0.2651, Validation Loss: 0.4256\n",
      "Epoch 76/100, Training Loss: 0.2613, Validation Loss: 0.4259\n",
      "Epoch 77/100, Training Loss: 0.2506, Validation Loss: 0.4250\n",
      "Epoch 78/100, Training Loss: 0.2654, Validation Loss: 0.4283\n",
      "Epoch 79/100, Training Loss: 0.2723, Validation Loss: 0.4265\n",
      "Epoch 80/100, Training Loss: 0.2551, Validation Loss: 0.4264\n",
      "Epoch 81/100, Training Loss: 0.2638, Validation Loss: 0.4254\n",
      "Epoch 82/100, Training Loss: 0.2570, Validation Loss: 0.4284\n",
      "Epoch 83/100, Training Loss: 0.2674, Validation Loss: 0.4261\n",
      "Epoch 84/100, Training Loss: 0.2715, Validation Loss: 0.4291\n",
      "Epoch 85/100, Training Loss: 0.2586, Validation Loss: 0.4224\n",
      "Epoch 86/100, Training Loss: 0.2534, Validation Loss: 0.4288\n",
      "Epoch 87/100, Training Loss: 0.2683, Validation Loss: 0.4256\n",
      "Epoch 88/100, Training Loss: 0.2617, Validation Loss: 0.4284\n",
      "Epoch 89/100, Training Loss: 0.2626, Validation Loss: 0.4291\n",
      "Epoch 90/100, Training Loss: 0.2575, Validation Loss: 0.4251\n",
      "Epoch 91/100, Training Loss: 0.2687, Validation Loss: 0.4248\n",
      "Epoch 92/100, Training Loss: 0.2557, Validation Loss: 0.4286\n",
      "Epoch 93/100, Training Loss: 0.2683, Validation Loss: 0.4229\n",
      "Epoch 94/100, Training Loss: 0.2603, Validation Loss: 0.4244\n",
      "Epoch 95/100, Training Loss: 0.2566, Validation Loss: 0.4247\n",
      "Epoch 96/100, Training Loss: 0.2562, Validation Loss: 0.4285\n",
      "Epoch 97/100, Training Loss: 0.2596, Validation Loss: 0.4312\n",
      "Epoch 98/100, Training Loss: 0.2619, Validation Loss: 0.4262\n",
      "Epoch 99/100, Training Loss: 0.2606, Validation Loss: 0.4268\n",
      "Epoch 100/100, Training Loss: 0.2641, Validation Loss: 0.4289\n",
      "Epoch 1/100, Training Loss: 0.7133, Validation Loss: 0.3513\n",
      "Epoch 2/100, Training Loss: 0.4731, Validation Loss: 0.2804\n",
      "Epoch 3/100, Training Loss: 0.4189, Validation Loss: 0.2638\n",
      "Epoch 4/100, Training Loss: 0.3905, Validation Loss: 0.2636\n",
      "Epoch 5/100, Training Loss: 0.3700, Validation Loss: 0.2466\n",
      "Epoch 6/100, Training Loss: 0.3696, Validation Loss: 0.2455\n",
      "Epoch 7/100, Training Loss: 0.3570, Validation Loss: 0.2524\n",
      "Epoch 8/100, Training Loss: 0.3684, Validation Loss: 0.2511\n",
      "Epoch 9/100, Training Loss: 0.3579, Validation Loss: 0.2542\n",
      "Epoch 10/100, Training Loss: 0.3485, Validation Loss: 0.2494\n",
      "Epoch 11/100, Training Loss: 0.3401, Validation Loss: 0.2492\n",
      "Epoch 12/100, Training Loss: 0.3409, Validation Loss: 0.2387\n",
      "Epoch 13/100, Training Loss: 0.3311, Validation Loss: 0.2384\n",
      "Epoch 14/100, Training Loss: 0.3467, Validation Loss: 0.2341\n",
      "Epoch 15/100, Training Loss: 0.3284, Validation Loss: 0.2378\n",
      "Epoch 16/100, Training Loss: 0.3410, Validation Loss: 0.2482\n",
      "Epoch 17/100, Training Loss: 0.3340, Validation Loss: 0.2346\n",
      "Epoch 18/100, Training Loss: 0.3278, Validation Loss: 0.2394\n",
      "Epoch 19/100, Training Loss: 0.3455, Validation Loss: 0.2416\n",
      "Epoch 20/100, Training Loss: 0.3313, Validation Loss: 0.2374\n",
      "Epoch 21/100, Training Loss: 0.3413, Validation Loss: 0.2320\n",
      "Epoch 22/100, Training Loss: 0.3352, Validation Loss: 0.2409\n",
      "Epoch 23/100, Training Loss: 0.3295, Validation Loss: 0.2342\n",
      "Epoch 24/100, Training Loss: 0.3293, Validation Loss: 0.2392\n",
      "Epoch 25/100, Training Loss: 0.3439, Validation Loss: 0.2369\n",
      "Epoch 26/100, Training Loss: 0.3352, Validation Loss: 0.2347\n",
      "Epoch 27/100, Training Loss: 0.3317, Validation Loss: 0.2328\n",
      "Epoch 28/100, Training Loss: 0.3276, Validation Loss: 0.2356\n",
      "Epoch 29/100, Training Loss: 0.3217, Validation Loss: 0.2431\n",
      "Epoch 30/100, Training Loss: 0.3326, Validation Loss: 0.2487\n",
      "Epoch 31/100, Training Loss: 0.3256, Validation Loss: 0.2394\n",
      "Epoch 32/100, Training Loss: 0.3311, Validation Loss: 0.2563\n",
      "Epoch 33/100, Training Loss: 0.3248, Validation Loss: 0.2433\n",
      "Epoch 34/100, Training Loss: 0.3302, Validation Loss: 0.2392\n",
      "Epoch 35/100, Training Loss: 0.3346, Validation Loss: 0.2356\n",
      "Epoch 36/100, Training Loss: 0.3341, Validation Loss: 0.2396\n",
      "Epoch 37/100, Training Loss: 0.3211, Validation Loss: 0.2459\n",
      "Epoch 38/100, Training Loss: 0.3285, Validation Loss: 0.2342\n",
      "Epoch 39/100, Training Loss: 0.3261, Validation Loss: 0.2411\n",
      "Epoch 40/100, Training Loss: 0.3227, Validation Loss: 0.2339\n",
      "Epoch 41/100, Training Loss: 0.3373, Validation Loss: 0.2391\n",
      "Epoch 42/100, Training Loss: 0.3232, Validation Loss: 0.2350\n",
      "Epoch 43/100, Training Loss: 0.3264, Validation Loss: 0.2426\n",
      "Epoch 44/100, Training Loss: 0.3249, Validation Loss: 0.2369\n",
      "Epoch 45/100, Training Loss: 0.3357, Validation Loss: 0.2444\n",
      "Epoch 46/100, Training Loss: 0.3298, Validation Loss: 0.2500\n",
      "Epoch 47/100, Training Loss: 0.3273, Validation Loss: 0.2356\n",
      "Epoch 48/100, Training Loss: 0.3205, Validation Loss: 0.2335\n",
      "Epoch 49/100, Training Loss: 0.3241, Validation Loss: 0.2351\n",
      "Epoch 50/100, Training Loss: 0.3275, Validation Loss: 0.2302\n",
      "Epoch 51/100, Training Loss: 0.3214, Validation Loss: 0.2325\n",
      "Epoch 52/100, Training Loss: 0.3241, Validation Loss: 0.2347\n",
      "Epoch 53/100, Training Loss: 0.3291, Validation Loss: 0.2448\n",
      "Epoch 54/100, Training Loss: 0.3278, Validation Loss: 0.2322\n",
      "Epoch 55/100, Training Loss: 0.3250, Validation Loss: 0.2453\n",
      "Epoch 56/100, Training Loss: 0.3222, Validation Loss: 0.2348\n",
      "Epoch 57/100, Training Loss: 0.3187, Validation Loss: 0.2324\n",
      "Epoch 58/100, Training Loss: 0.3350, Validation Loss: 0.2306\n",
      "Epoch 59/100, Training Loss: 0.3319, Validation Loss: 0.2389\n",
      "Epoch 60/100, Training Loss: 0.3293, Validation Loss: 0.2276\n",
      "Epoch 61/100, Training Loss: 0.3179, Validation Loss: 0.2320\n",
      "Epoch 62/100, Training Loss: 0.3280, Validation Loss: 0.2292\n",
      "Epoch 63/100, Training Loss: 0.3247, Validation Loss: 0.2450\n",
      "Epoch 64/100, Training Loss: 0.3250, Validation Loss: 0.2405\n",
      "Epoch 65/100, Training Loss: 0.3323, Validation Loss: 0.2331\n",
      "Epoch 66/100, Training Loss: 0.3204, Validation Loss: 0.2337\n",
      "Epoch 67/100, Training Loss: 0.3249, Validation Loss: 0.2376\n",
      "Epoch 68/100, Training Loss: 0.3215, Validation Loss: 0.2316\n",
      "Epoch 69/100, Training Loss: 0.3284, Validation Loss: 0.2457\n",
      "Epoch 70/100, Training Loss: 0.3274, Validation Loss: 0.2365\n",
      "Epoch 71/100, Training Loss: 0.3237, Validation Loss: 0.2363\n",
      "Epoch 72/100, Training Loss: 0.3191, Validation Loss: 0.2406\n",
      "Epoch 73/100, Training Loss: 0.3266, Validation Loss: 0.2320\n",
      "Epoch 74/100, Training Loss: 0.3241, Validation Loss: 0.2407\n",
      "Epoch 75/100, Training Loss: 0.3209, Validation Loss: 0.2320\n",
      "Epoch 76/100, Training Loss: 0.3261, Validation Loss: 0.2299\n",
      "Epoch 77/100, Training Loss: 0.3221, Validation Loss: 0.2317\n",
      "Epoch 78/100, Training Loss: 0.3262, Validation Loss: 0.2269\n",
      "Epoch 79/100, Training Loss: 0.3166, Validation Loss: 0.2337\n",
      "Epoch 80/100, Training Loss: 0.3184, Validation Loss: 0.2385\n",
      "Epoch 81/100, Training Loss: 0.3261, Validation Loss: 0.2375\n",
      "Epoch 82/100, Training Loss: 0.3175, Validation Loss: 0.2343\n",
      "Epoch 83/100, Training Loss: 0.3260, Validation Loss: 0.2359\n",
      "Epoch 84/100, Training Loss: 0.3153, Validation Loss: 0.2346\n",
      "Epoch 85/100, Training Loss: 0.3181, Validation Loss: 0.2337\n",
      "Epoch 86/100, Training Loss: 0.3273, Validation Loss: 0.2336\n",
      "Epoch 87/100, Training Loss: 0.3246, Validation Loss: 0.2360\n",
      "Epoch 88/100, Training Loss: 0.3200, Validation Loss: 0.2340\n",
      "Epoch 89/100, Training Loss: 0.3270, Validation Loss: 0.2417\n",
      "Epoch 90/100, Training Loss: 0.3246, Validation Loss: 0.2300\n",
      "Epoch 91/100, Training Loss: 0.3210, Validation Loss: 0.2335\n",
      "Epoch 92/100, Training Loss: 0.3158, Validation Loss: 0.2371\n",
      "Epoch 93/100, Training Loss: 0.3219, Validation Loss: 0.2323\n",
      "Epoch 94/100, Training Loss: 0.3231, Validation Loss: 0.2334\n",
      "Epoch 95/100, Training Loss: 0.3172, Validation Loss: 0.2257\n",
      "Epoch 96/100, Training Loss: 0.3228, Validation Loss: 0.2297\n",
      "Epoch 97/100, Training Loss: 0.3276, Validation Loss: 0.2315\n",
      "Epoch 98/100, Training Loss: 0.3260, Validation Loss: 0.2323\n",
      "Epoch 99/100, Training Loss: 0.3268, Validation Loss: 0.2310\n",
      "Epoch 100/100, Training Loss: 0.3191, Validation Loss: 0.2387\n",
      "Epoch 1/100, Training Loss: 0.6208, Validation Loss: 0.2808\n",
      "Epoch 2/100, Training Loss: 0.3630, Validation Loss: 0.2516\n",
      "Epoch 3/100, Training Loss: 0.3326, Validation Loss: 0.2360\n",
      "Epoch 4/100, Training Loss: 0.3256, Validation Loss: 0.2395\n",
      "Epoch 5/100, Training Loss: 0.3137, Validation Loss: 0.2359\n",
      "Epoch 6/100, Training Loss: 0.3171, Validation Loss: 0.2399\n",
      "Epoch 7/100, Training Loss: 0.3076, Validation Loss: 0.2366\n",
      "Epoch 8/100, Training Loss: 0.3149, Validation Loss: 0.2419\n",
      "Epoch 9/100, Training Loss: 0.3085, Validation Loss: 0.2406\n",
      "Epoch 10/100, Training Loss: 0.3019, Validation Loss: 0.2390\n",
      "Epoch 11/100, Training Loss: 0.2998, Validation Loss: 0.2382\n",
      "Epoch 12/100, Training Loss: 0.3018, Validation Loss: 0.2342\n",
      "Epoch 13/100, Training Loss: 0.2971, Validation Loss: 0.2349\n",
      "Epoch 14/100, Training Loss: 0.3050, Validation Loss: 0.2360\n",
      "Epoch 15/100, Training Loss: 0.2937, Validation Loss: 0.2422\n",
      "Epoch 16/100, Training Loss: 0.2988, Validation Loss: 0.2367\n",
      "Epoch 17/100, Training Loss: 0.2952, Validation Loss: 0.2320\n",
      "Epoch 18/100, Training Loss: 0.2928, Validation Loss: 0.2327\n",
      "Epoch 19/100, Training Loss: 0.3039, Validation Loss: 0.2316\n",
      "Epoch 20/100, Training Loss: 0.2995, Validation Loss: 0.2335\n",
      "Epoch 21/100, Training Loss: 0.2988, Validation Loss: 0.2316\n",
      "Epoch 22/100, Training Loss: 0.2989, Validation Loss: 0.2315\n",
      "Epoch 23/100, Training Loss: 0.2898, Validation Loss: 0.2397\n",
      "Epoch 24/100, Training Loss: 0.2943, Validation Loss: 0.2345\n",
      "Epoch 25/100, Training Loss: 0.2953, Validation Loss: 0.2334\n",
      "Epoch 26/100, Training Loss: 0.2930, Validation Loss: 0.2309\n",
      "Epoch 27/100, Training Loss: 0.2930, Validation Loss: 0.2341\n",
      "Epoch 28/100, Training Loss: 0.2942, Validation Loss: 0.2339\n",
      "Epoch 29/100, Training Loss: 0.2932, Validation Loss: 0.2350\n",
      "Epoch 30/100, Training Loss: 0.2910, Validation Loss: 0.2318\n",
      "Epoch 31/100, Training Loss: 0.2898, Validation Loss: 0.2353\n",
      "Epoch 32/100, Training Loss: 0.2951, Validation Loss: 0.2363\n",
      "Epoch 33/100, Training Loss: 0.2913, Validation Loss: 0.2351\n",
      "Epoch 34/100, Training Loss: 0.2927, Validation Loss: 0.2320\n",
      "Epoch 35/100, Training Loss: 0.2929, Validation Loss: 0.2330\n",
      "Epoch 36/100, Training Loss: 0.2905, Validation Loss: 0.2366\n",
      "Epoch 37/100, Training Loss: 0.2931, Validation Loss: 0.2339\n",
      "Epoch 38/100, Training Loss: 0.2936, Validation Loss: 0.2305\n",
      "Epoch 39/100, Training Loss: 0.2874, Validation Loss: 0.2366\n",
      "Epoch 40/100, Training Loss: 0.2912, Validation Loss: 0.2296\n",
      "Epoch 41/100, Training Loss: 0.2992, Validation Loss: 0.2376\n",
      "Epoch 42/100, Training Loss: 0.2893, Validation Loss: 0.2339\n",
      "Epoch 43/100, Training Loss: 0.2878, Validation Loss: 0.2317\n",
      "Epoch 44/100, Training Loss: 0.2954, Validation Loss: 0.2302\n",
      "Epoch 45/100, Training Loss: 0.2910, Validation Loss: 0.2339\n",
      "Epoch 46/100, Training Loss: 0.2937, Validation Loss: 0.2326\n",
      "Epoch 47/100, Training Loss: 0.2959, Validation Loss: 0.2309\n",
      "Epoch 48/100, Training Loss: 0.2888, Validation Loss: 0.2320\n",
      "Epoch 49/100, Training Loss: 0.2897, Validation Loss: 0.2294\n",
      "Epoch 50/100, Training Loss: 0.2910, Validation Loss: 0.2306\n",
      "Epoch 51/100, Training Loss: 0.2894, Validation Loss: 0.2348\n",
      "Epoch 52/100, Training Loss: 0.2940, Validation Loss: 0.2320\n",
      "Epoch 53/100, Training Loss: 0.2916, Validation Loss: 0.2454\n",
      "Epoch 54/100, Training Loss: 0.2887, Validation Loss: 0.2316\n",
      "Epoch 55/100, Training Loss: 0.2898, Validation Loss: 0.2452\n",
      "Epoch 56/100, Training Loss: 0.2891, Validation Loss: 0.2357\n",
      "Epoch 57/100, Training Loss: 0.2897, Validation Loss: 0.2313\n",
      "Epoch 58/100, Training Loss: 0.2926, Validation Loss: 0.2281\n",
      "Epoch 59/100, Training Loss: 0.2906, Validation Loss: 0.2353\n",
      "Epoch 60/100, Training Loss: 0.2919, Validation Loss: 0.2268\n",
      "Epoch 61/100, Training Loss: 0.2856, Validation Loss: 0.2310\n",
      "Epoch 62/100, Training Loss: 0.2895, Validation Loss: 0.2290\n",
      "Epoch 63/100, Training Loss: 0.2900, Validation Loss: 0.2462\n",
      "Epoch 64/100, Training Loss: 0.2878, Validation Loss: 0.2376\n",
      "Epoch 65/100, Training Loss: 0.2989, Validation Loss: 0.2312\n",
      "Epoch 66/100, Training Loss: 0.2865, Validation Loss: 0.2319\n",
      "Epoch 67/100, Training Loss: 0.2895, Validation Loss: 0.2311\n",
      "Epoch 68/100, Training Loss: 0.2838, Validation Loss: 0.2285\n",
      "Epoch 69/100, Training Loss: 0.2946, Validation Loss: 0.2338\n",
      "Epoch 70/100, Training Loss: 0.2883, Validation Loss: 0.2329\n",
      "Epoch 71/100, Training Loss: 0.2891, Validation Loss: 0.2358\n",
      "Epoch 72/100, Training Loss: 0.2869, Validation Loss: 0.2421\n",
      "Epoch 73/100, Training Loss: 0.2916, Validation Loss: 0.2294\n",
      "Epoch 74/100, Training Loss: 0.2866, Validation Loss: 0.2305\n",
      "Epoch 75/100, Training Loss: 0.2871, Validation Loss: 0.2371\n",
      "Epoch 76/100, Training Loss: 0.2966, Validation Loss: 0.2296\n",
      "Epoch 77/100, Training Loss: 0.2879, Validation Loss: 0.2376\n",
      "Epoch 78/100, Training Loss: 0.2879, Validation Loss: 0.2335\n",
      "Epoch 79/100, Training Loss: 0.2842, Validation Loss: 0.2324\n",
      "Epoch 80/100, Training Loss: 0.2843, Validation Loss: 0.2362\n",
      "Epoch 81/100, Training Loss: 0.2868, Validation Loss: 0.2351\n",
      "Epoch 82/100, Training Loss: 0.2876, Validation Loss: 0.2323\n",
      "Epoch 83/100, Training Loss: 0.2897, Validation Loss: 0.2310\n",
      "Epoch 84/100, Training Loss: 0.2841, Validation Loss: 0.2352\n",
      "Epoch 85/100, Training Loss: 0.2868, Validation Loss: 0.2366\n",
      "Epoch 86/100, Training Loss: 0.2965, Validation Loss: 0.2381\n",
      "Epoch 87/100, Training Loss: 0.2943, Validation Loss: 0.2318\n",
      "Epoch 88/100, Training Loss: 0.2861, Validation Loss: 0.2314\n",
      "Epoch 89/100, Training Loss: 0.2922, Validation Loss: 0.2343\n",
      "Epoch 90/100, Training Loss: 0.2936, Validation Loss: 0.2289\n",
      "Epoch 91/100, Training Loss: 0.2861, Validation Loss: 0.2291\n",
      "Epoch 92/100, Training Loss: 0.2882, Validation Loss: 0.2344\n",
      "Epoch 93/100, Training Loss: 0.2811, Validation Loss: 0.2309\n",
      "Epoch 94/100, Training Loss: 0.2814, Validation Loss: 0.2298\n",
      "Epoch 95/100, Training Loss: 0.2894, Validation Loss: 0.2276\n",
      "Epoch 96/100, Training Loss: 0.2893, Validation Loss: 0.2301\n",
      "Epoch 97/100, Training Loss: 0.2876, Validation Loss: 0.2334\n",
      "Epoch 98/100, Training Loss: 0.2919, Validation Loss: 0.2287\n",
      "Epoch 99/100, Training Loss: 0.2888, Validation Loss: 0.2297\n",
      "Epoch 100/100, Training Loss: 0.2891, Validation Loss: 0.2326\n",
      "Epoch 1/100, Training Loss: 0.6745, Validation Loss: 0.5110\n",
      "Epoch 2/100, Training Loss: 0.4114, Validation Loss: 0.4568\n",
      "Epoch 3/100, Training Loss: 0.3612, Validation Loss: 0.4462\n",
      "Epoch 4/100, Training Loss: 0.3336, Validation Loss: 0.4351\n",
      "Epoch 5/100, Training Loss: 0.3206, Validation Loss: 0.4270\n",
      "Epoch 6/100, Training Loss: 0.3072, Validation Loss: 0.4216\n",
      "Epoch 7/100, Training Loss: 0.3056, Validation Loss: 0.4261\n",
      "Epoch 8/100, Training Loss: 0.3041, Validation Loss: 0.4257\n",
      "Epoch 9/100, Training Loss: 0.2962, Validation Loss: 0.4161\n",
      "Epoch 10/100, Training Loss: 0.2990, Validation Loss: 0.4163\n",
      "Epoch 11/100, Training Loss: 0.2974, Validation Loss: 0.4264\n",
      "Epoch 12/100, Training Loss: 0.2878, Validation Loss: 0.4212\n",
      "Epoch 13/100, Training Loss: 0.2852, Validation Loss: 0.4174\n",
      "Epoch 14/100, Training Loss: 0.2835, Validation Loss: 0.4143\n",
      "Epoch 15/100, Training Loss: 0.2921, Validation Loss: 0.4197\n",
      "Epoch 16/100, Training Loss: 0.2869, Validation Loss: 0.4193\n",
      "Epoch 17/100, Training Loss: 0.2810, Validation Loss: 0.4205\n",
      "Epoch 18/100, Training Loss: 0.2771, Validation Loss: 0.4165\n",
      "Epoch 19/100, Training Loss: 0.2841, Validation Loss: 0.4174\n",
      "Epoch 20/100, Training Loss: 0.2868, Validation Loss: 0.4181\n",
      "Epoch 21/100, Training Loss: 0.2838, Validation Loss: 0.4197\n",
      "Epoch 22/100, Training Loss: 0.2886, Validation Loss: 0.4207\n",
      "Epoch 23/100, Training Loss: 0.2814, Validation Loss: 0.4204\n",
      "Epoch 24/100, Training Loss: 0.2822, Validation Loss: 0.4252\n",
      "Epoch 25/100, Training Loss: 0.2753, Validation Loss: 0.4224\n",
      "Epoch 26/100, Training Loss: 0.2796, Validation Loss: 0.4181\n",
      "Epoch 27/100, Training Loss: 0.2785, Validation Loss: 0.4186\n",
      "Epoch 28/100, Training Loss: 0.2849, Validation Loss: 0.4257\n",
      "Epoch 29/100, Training Loss: 0.2863, Validation Loss: 0.4178\n",
      "Epoch 30/100, Training Loss: 0.2807, Validation Loss: 0.4169\n",
      "Epoch 31/100, Training Loss: 0.2871, Validation Loss: 0.4225\n",
      "Epoch 32/100, Training Loss: 0.2751, Validation Loss: 0.4149\n",
      "Epoch 33/100, Training Loss: 0.2763, Validation Loss: 0.4170\n",
      "Epoch 34/100, Training Loss: 0.2778, Validation Loss: 0.4133\n",
      "Epoch 35/100, Training Loss: 0.2733, Validation Loss: 0.4112\n",
      "Epoch 36/100, Training Loss: 0.2750, Validation Loss: 0.4180\n",
      "Epoch 37/100, Training Loss: 0.2782, Validation Loss: 0.4215\n",
      "Epoch 38/100, Training Loss: 0.2864, Validation Loss: 0.4203\n",
      "Epoch 39/100, Training Loss: 0.2843, Validation Loss: 0.4148\n",
      "Epoch 40/100, Training Loss: 0.2709, Validation Loss: 0.4179\n",
      "Epoch 41/100, Training Loss: 0.2846, Validation Loss: 0.4156\n",
      "Epoch 42/100, Training Loss: 0.2778, Validation Loss: 0.4150\n",
      "Epoch 43/100, Training Loss: 0.2816, Validation Loss: 0.4144\n",
      "Epoch 44/100, Training Loss: 0.2716, Validation Loss: 0.4132\n",
      "Epoch 45/100, Training Loss: 0.2744, Validation Loss: 0.4303\n",
      "Epoch 46/100, Training Loss: 0.2724, Validation Loss: 0.4201\n",
      "Epoch 47/100, Training Loss: 0.2725, Validation Loss: 0.4196\n",
      "Epoch 48/100, Training Loss: 0.2720, Validation Loss: 0.4145\n",
      "Epoch 49/100, Training Loss: 0.2740, Validation Loss: 0.4194\n",
      "Epoch 50/100, Training Loss: 0.2837, Validation Loss: 0.4123\n",
      "Epoch 51/100, Training Loss: 0.2726, Validation Loss: 0.4162\n",
      "Epoch 52/100, Training Loss: 0.2646, Validation Loss: 0.4147\n",
      "Epoch 53/100, Training Loss: 0.2740, Validation Loss: 0.4237\n",
      "Epoch 54/100, Training Loss: 0.2660, Validation Loss: 0.4167\n",
      "Epoch 55/100, Training Loss: 0.2651, Validation Loss: 0.4194\n",
      "Epoch 56/100, Training Loss: 0.2704, Validation Loss: 0.4132\n",
      "Epoch 57/100, Training Loss: 0.2690, Validation Loss: 0.4151\n",
      "Epoch 58/100, Training Loss: 0.2739, Validation Loss: 0.4141\n",
      "Epoch 59/100, Training Loss: 0.2663, Validation Loss: 0.4241\n",
      "Epoch 60/100, Training Loss: 0.2821, Validation Loss: 0.4137\n",
      "Epoch 61/100, Training Loss: 0.2681, Validation Loss: 0.4090\n",
      "Epoch 62/100, Training Loss: 0.2675, Validation Loss: 0.4100\n",
      "Epoch 63/100, Training Loss: 0.2653, Validation Loss: 0.4127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m histories = []       \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m grid_search(student_model_param):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     hist, student_model_ = \u001b[43mstudent_model_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmode\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom_state\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_batch_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_batch_norm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     histories.append(hist)\n\u001b[32m      6\u001b[39m json.dump(histories, \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mresults/student_histories.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mstudent_model_training\u001b[39m\u001b[34m(student_df, learning_rate, num_epochs, batch_size, mode, random_state, use_batch_norm)\u001b[39m\n\u001b[32m     13\u001b[39m val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     14\u001b[39m test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m test_results = evaluate_model(student_model_, test_loader, device,loss_type=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mfinal_test_loss\u001b[39m\u001b[33m'\u001b[39m] = test_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ISF/DL/Benchmark-impact-of-activation-function/training_functions.py:39\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, num_epochs, train_loader, val_loader, device)\u001b[39m\n\u001b[32m     37\u001b[39m     loss = criterion(outputs, targets)\n\u001b[32m     38\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     running_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     42\u001b[39m epoch_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ISF/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ISF/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ISF/.venv/lib/python3.11/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ISF/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ISF/.venv/lib/python3.11/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ISF/.venv/lib/python3.11/site-packages/torch/optim/adam.py:420\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    418\u001b[39m                 grad = grad.add(param, alpha=weight_decay)\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m             grad = \u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_complex(param):\n\u001b[32m    423\u001b[39m     grad = torch.view_as_real(grad)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_STUDENTS:\n",
    "    histories = []       \n",
    "    for param in grid_search(student_model_param):\n",
    "        hist, student_model_ = student_model_training(student_df, learning_rate=0.001, num_epochs=100, batch_size=param['batch_size'], mode=param['mode'], random_state=param['random_state'], use_batch_norm=param['use_batch_norm'])\n",
    "        histories.append(hist)\n",
    "    json.dump(histories, open('results/student_histories.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food price inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation_df = pd.read_csv('data/umitka/food-price-inflation/food_price_inflation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def food_price_model_training(inflation_df, learning_rate, num_epochs, batch_size, mode, random_state, use_batch_norm):\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    train_set, val_set, test_set, scaler_X, scaler_y = preprocess(\n",
    "        inflation_df, \n",
    "        split_ratio_train_test=0.8, \n",
    "        split_ratio_train_valid=0.8, device=device)\n",
    "    input_size = train_set.tensors[0].shape[2]\n",
    "    \n",
    "    inflation_model = lstm(input_size=input_size, mode = mode,use_batch_norm=use_batch_norm).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(inflation_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "    history = train_model(inflation_model, criterion, optimizer, num_epochs,train_loader, val_loader,device)\n",
    "    test_results = evaluate_model(inflation_model, test_loader, device, loss_type= 'mse')\n",
    "    history['final_test_loss'] = test_results\n",
    "    history['dataset'] = 'food_price'\n",
    "    history['random_state'] = random_state\n",
    "\n",
    "    return history, inflation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_price_model_param = {\n",
    "    'mode' : ['relu','gelu'],\n",
    "    'batch_size' : [32,1024],\n",
    "    'random_state' : [1,2,3],\n",
    "    'use_batch_norm' : [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_FOOD:\n",
    "    histories = []\n",
    "    for param in grid_search(food_price_model_param):\n",
    "        hist, inflation_model_ = food_price_model_training(inflation_df, learning_rate=0.001, num_epochs=100, batch_size=param['batch_size'], mode=param['mode'], random_state=param['random_state'], use_batch_norm=param['use_batch_norm'])\n",
    "        histories.append(hist)\n",
    "    json.dump(histories, open('results/food_price_histories.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
